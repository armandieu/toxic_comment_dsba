{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Toxic comment classification"
      ],
      "metadata": {
        "id": "DAX3ibMe-lZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq88HwVYYDHB",
        "outputId": "48df27d3-3b7f-44fe-cb1f-eecfca488473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bkAdPWLCX5aN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from torchmetrics import AUROC, F1Score\n",
        "from sklearn.metrics import classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchtext.transforms as T\n",
        "import torchdata.datapipes as dp\n",
        "from torchtext import data\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import spacy\n",
        "import random\n",
        "import collections\n",
        "import time\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler,SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "av_sGp5opq0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c92901b-38b3-410a-fc12-3d09e70adaa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive = drive.mount('/content/drive')\n",
        "    data_dir = '/content/drive/My Drive/Dsba/04 M2 T1/Foundations of Deep Learning/Kaggle Competiton/toxic-comment-classification-dsba-2023/kaggle_data'\n",
        "    drive_dir = '/content/drive/My Drive/Dsba/04 M2 T1/Foundations of Deep Learning/Kaggle Competiton/toxic-comment-classification-dsba-2023/'\n",
        "    idle='gdrive'\n",
        "except:\n",
        "    idle='kaggle'\n",
        "    data_dir = '/kaggle/input/toxic-comment-data/'\n",
        "    drive_dir = '/kaggle/working/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kbJgptfTNll",
        "outputId": "3e01dc2f-2f3b-4e85-aef6-25e3f7ab345e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# CPU or GPU device ?\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWvjXjZxZ42a",
        "outputId": "690616ec-fd4c-4a73-ff01-5c3df4ea8569"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7acb5c1f2590>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "hDCMnDqswi6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_models = {\n",
        "    'bert0': '/models/BERT/bert_model_V0.bin',\n",
        "    'distilbert': '/models/BERT/DISTBert_model_epoch0.bin',\n",
        "    'lstm0':'/models/LSTM/LSTM.bin',\n",
        "    'lstm_yj':'/models/LSTM/LSTM_YJ.bin',\n",
        "    'convnn0':'/models/CONVNN/kaggle_dataconvnn_2-model.pt'\n",
        "}\n",
        "model_params = {\n",
        "    'bert0': {\n",
        "        'MAX_SEQ_LENGTH': 220,\n",
        "        'BATCH_SIZE': 32,\n",
        "        'GRADIENT_ACCUMULATION_STEPS' : 1,\n",
        "        'NUM_TRAIN_EPOCHS' : 10,\n",
        "        'LEARNING_RATE' : 5e-5,\n",
        "        'WARMUP_PROPORTION' : 0.1,\n",
        "        'MAX_GRAD_NORM' : 5,\n",
        "        'NUM_WORKERS' : 2,\n",
        "        'PATIENCE': 3\n",
        "\n",
        "    },\n",
        "    'distilbert': {\n",
        "        'MAX_SEQ_LENGTH': 512,\n",
        "        'BATCH_SIZE': 16,\n",
        "        'GRADIENT_ACCUMULATION_STEPS' : 1,\n",
        "        'NUM_TRAIN_EPOCHS' : 2,\n",
        "        'WARMUP_PROPORTION' : 0.1,\n",
        "        'MAX_GRAD_NORM' : 5,\n",
        "        'PATIENCE' :2,\n",
        "        'MAX_LEN' : 512,\n",
        "        'TRAIN_BATCH_SIZE' : 16,\n",
        "        'EPOCHS' : 2,\n",
        "        'LEARNING_RATE' : 1e-05,\n",
        "        'NUM_WORKERS' : 2,\n",
        "    },\n",
        "    'convnn0':\n",
        "     {\n",
        "    'INPUT_DIM': 2500,\n",
        "    'EMBEDDING_DIM': 100,\n",
        "    'N_FILTERS': 100,\n",
        "    'FILTER_SIZES': [3,4,5],\n",
        "    'OUTPUT_DIM': 1,\n",
        "    'DROPOUT':0.5,\n",
        "    'LR': 5e-4\n",
        "    },\n",
        "    'lstm0':\n",
        "     {\n",
        "        'INPUT_DIM':40000,\n",
        "        'EMBEDDING_DIM': 200,\n",
        "        'HIDDEN_DIM':15,\n",
        "        'N_LAYERS':1,\n",
        "        'OUTPUT_DIM':1,\n",
        "        'DROPOUT':0.5,\n",
        "        'LR':1e-4,\n",
        "        'BS': 64,\n",
        "        'BI':False\n",
        "    },\n",
        "    'lstm_yj':\n",
        "     {\n",
        "        'INPUT_DIM':40000,\n",
        "        'EMBEDDING_DIM': 300,\n",
        "        'HIDDEN_DIM':45,\n",
        "        'N_LAYERS':2,\n",
        "        'OUTPUT_DIM':1,\n",
        "        'DROPOUT':0.5,\n",
        "        'LR':1e-4,\n",
        "        'BS': 64,\n",
        "        'N_EPOCHS' : 5,\n",
        "        'BI':False\n",
        "    }\n",
        "                }\n",
        "note_book_config ={\n",
        "    'generate_augmented': False\n",
        "}\n",
        "args_denoise_custom = {\n",
        "    'strip_html': False,\n",
        "    'remove_stop':True,\n",
        "    'stem_word': False,\n",
        "    'remove_patterns': True,\n",
        "    'lem_word': False,\n",
        "}\n",
        "models ={}"
      ],
      "metadata": {
        "id": "uQ4fhYjYhDHn"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxdgY0c3Jc7f"
      },
      "source": [
        "### Data cleaning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwDCCearon4x",
        "outputId": "c07d4f34-b838-4f3d-a697-7df2127710e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stop_faster(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  return ' '.join(filtered_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_string(s):\n",
        "    # Replace all runs of whitespaces with no space\n",
        "    s = re.sub(r\"\\s+\", '', s)\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "7PeOe6o_Fmt3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "66JPrFAPWT_7"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def remove_stop_w(text):\n",
        "  doc = nlp(text)\n",
        "  filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "  clean_text = ' '.join(filtered_tokens)\n",
        "  return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HMtxrh1YJebB"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def strip_html(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  return soup.get_text()\n",
        "import re\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "  return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "  text = strip_html(text)\n",
        "  # print(text)\n",
        "  # text = remove_between_square_brackets(text)\n",
        "  text = remove_special_characters(text)\n",
        "  text = remove_brackets(text)\n",
        "  text = remove_hyperlinks(text)\n",
        "  text = remove_hashtags(text)\n",
        "  text = preprocess_string(text)\n",
        "  return text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "  pattern=r'[^a-zA-z0-9\\s]'\n",
        "  text=re.sub(pattern,'',text)\n",
        "  return text\n",
        "\n",
        "def remove_brackets(text):\n",
        "    # Define the regular expression pattern to match square brackets and their contents\n",
        "    pattern = r'[\\[\\]]'\n",
        "\n",
        "    # Use re.sub to replace matches with an empty string\n",
        "    result = re.sub(pattern, '', text)\n",
        "\n",
        "    return result\n",
        "def remove_hyperlinks(text):\n",
        "    # Remove hyperlinks using a regular expression\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    # Remove hashtags using a regular expression\n",
        "    return re.sub(r'#\\S+', '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIvF_tCDjwOR"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I2aXR27S9WoL"
      },
      "outputs": [],
      "source": [
        "def worst_group_accuracy(prediction, y, include_cr=False):\n",
        "    \"\"\"\n",
        "        Compute the worst group accuracy, with the groups being defined by ['male', 'female', 'LGBTQ',\n",
        "        'christian', 'muslim', 'other_religions', 'black', 'white'] for positive and negative toxicity.\n",
        "        arguments:\n",
        "            prediction [pandas.DataFrame]: dataframe with 2 columns (index and pred)\n",
        "            y [pandas.DataFrame]: dataframe containing the metadata\n",
        "        returns:\n",
        "            wga [float]: worst group accuracy\n",
        "    \"\"\"\n",
        "    # print(y)\n",
        "    y.loc[prediction.index, 'pred'] = prediction.pred\n",
        "\n",
        "    categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
        "    accuracies = []\n",
        "    for category in categories:\n",
        "        for label in [0, 1]:\n",
        "            group = y.loc[y[category] == label]\n",
        "            group_accuracy = (group['y'] == (group['pred'] > 0.5)).mean()\n",
        "            accuracies.append(group_accuracy)\n",
        "    if include_cr:\n",
        "      for category in categories:\n",
        "        for label in [0, 1]:\n",
        "                  group = y.loc[y[category] == label]\n",
        "                  group_preds = (group['pred'] > 0.5)\n",
        "                  # Generate and print the classification report\n",
        "                  report = classification_report(group['y'], group_preds)\n",
        "                  print(f\"Category: {category}_{label}\")\n",
        "                  print(report)\n",
        "                  print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "    wga = np.min(accuracies)\n",
        "    return wga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fKqdWcpJbSe3"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, criterion, cr =False):\n",
        "    \"\"\"\n",
        "        Evaluate the model on a given dataloader.\n",
        "        argument:\n",
        "            model [torch.nn.Module]: model to evaluate\n",
        "            dataloader [torch.utils.data.DataLoader]: dataloader on which to evaluate\n",
        "            criterion [torch.nn.modules.loss]: desired loss to compute\n",
        "        returns:\n",
        "            dataset_loss [float]: computed loss on the dataset\n",
        "            dataset_metric [float]: computed metric on the dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses, predictions, indices = [], [], []\n",
        "    for x, y, idx in tqdm(dataloader, leave=False):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(x)\n",
        "        loss = criterion(pred.squeeze(), y.squeeze().float())\n",
        "        losses.extend([loss.item()] * len(y))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idx.tolist())\n",
        "\n",
        "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "    dataset_loss = np.mean(losses)\n",
        "    dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy(), cr)\n",
        "    return dataset_loss, dataset_metric, pred_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy4-NEBAV71v"
      },
      "source": [
        "## Miscelaneous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rX5zNnJmjkI1"
      },
      "outputs": [],
      "source": [
        "categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnTnAui0T_F"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4chRFT0OozZ"
      },
      "source": [
        "#### Class loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom class loss for imbalanced categories"
      ],
      "metadata": {
        "id": "4vL2W5Jdycvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_xZeGOU2Ojye"
      },
      "outputs": [],
      "source": [
        "class LossComputer:\n",
        "    def __init__(self, criterion, is_robust, dataset, alpha=None, gamma=0.1, adj=None, min_var_weight=0, step_size=0.01, normalize_loss=False, btl=False):\n",
        "        self.groups = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
        "        self.criterion = criterion\n",
        "        self.is_robust = is_robust\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.min_var_weight = min_var_weight\n",
        "        self.step_size = step_size\n",
        "        self.normalize_loss = normalize_loss\n",
        "        self.btl = btl\n",
        "\n",
        "        self.n_groups = torch.tensor(8).to(device) #dataset.n_groups\n",
        "        self.group_counts = torch.tensor([dataset[dataset[group]==1].shape[0] for group in self.groups]).to(device) # dataset.group_counts().to(device)\n",
        "        self.group_frac = self.group_counts/self.group_counts.sum()\n",
        "        self.group_str = self.groups #dataset.group_str\n",
        "\n",
        "        if adj is not None:\n",
        "            self.adj = torch.from_numpy(adj).float().to(device)\n",
        "        else:\n",
        "            self.adj = torch.zeros(self.n_groups).float().to(device)\n",
        "\n",
        "        if is_robust:\n",
        "            assert alpha, 'alpha must be specified'\n",
        "\n",
        "        # quantities maintained throughout training\n",
        "        self.adv_probs = torch.ones(self.n_groups).to(device)/self.n_groups\n",
        "        self.exp_avg_loss = torch.zeros(self.n_groups).to(device)\n",
        "        self.exp_avg_initialized = torch.zeros(self.n_groups).byte().to(device)\n",
        "\n",
        "        self.reset_stats()\n",
        "\n",
        "    def loss(self, yhat, y, group_idx=None, is_training=False):\n",
        "        # compute per-sample and per-group losses\n",
        "        per_sample_losses = self.criterion(yhat, y)\n",
        "        group_loss, group_count = self.compute_group_avg(per_sample_losses, group_idx)\n",
        "        # group_acc, group_count = self.compute_group_avg((torch.argmax(yhat,1)==y).float(), group_idx)\n",
        "        group_acc, group_count = self.compute_group_avg(((yhat>0.5) ==y).float(), group_idx)\n",
        "        # print(group_loss)\n",
        "        # update historical losses\n",
        "        self.update_exp_avg_loss(group_loss, group_count)\n",
        "\n",
        "        # compute overall loss\n",
        "        if self.is_robust and not self.btl:\n",
        "            actual_loss, weights = self.compute_robust_loss(group_loss, group_count)\n",
        "        elif self.is_robust and self.btl:\n",
        "             actual_loss, weights = self.compute_robust_loss_btl(group_loss, group_count)\n",
        "        else:\n",
        "            actual_loss = per_sample_losses.mean()\n",
        "            weights = None\n",
        "\n",
        "        # update stats\n",
        "        self.update_stats(actual_loss, group_loss, group_acc, group_count, weights)\n",
        "\n",
        "        return actual_loss\n",
        "\n",
        "    def compute_robust_loss(self, group_loss, group_count):\n",
        "        adjusted_loss = group_loss\n",
        "        if torch.all(self.adj>0):\n",
        "            adjusted_loss += self.adj/torch.sqrt(self.group_counts)\n",
        "        if self.normalize_loss:\n",
        "            adjusted_loss = adjusted_loss/(adjusted_loss.sum())\n",
        "        self.adv_probs = self.adv_probs * torch.exp(self.step_size*adjusted_loss.data)\n",
        "        self.adv_probs = self.adv_probs/(self.adv_probs.sum())\n",
        "\n",
        "        robust_loss = group_loss @ self.adv_probs\n",
        "        return robust_loss, self.adv_probs\n",
        "\n",
        "    def compute_robust_loss_btl(self, group_loss, group_count):\n",
        "        adjusted_loss = self.exp_avg_loss + self.adj/torch.sqrt(self.group_counts)\n",
        "        return self.compute_robust_loss_greedy(group_loss, adjusted_loss)\n",
        "\n",
        "    def compute_robust_loss_greedy(self, group_loss, ref_loss):\n",
        "        sorted_idx = ref_loss.sort(descending=True)[1]\n",
        "        sorted_loss = group_loss[sorted_idx]\n",
        "        sorted_frac = self.group_frac[sorted_idx]\n",
        "\n",
        "        mask = torch.cumsum(sorted_frac, dim=0)<=self.alpha\n",
        "        weights = mask.float() * sorted_frac /self.alpha\n",
        "        last_idx = mask.sum()\n",
        "        weights[last_idx] = 1 - weights.sum()\n",
        "        weights = sorted_frac*self.min_var_weight + weights*(1-self.min_var_weight)\n",
        "\n",
        "        robust_loss = sorted_loss @ weights\n",
        "\n",
        "        # sort the weights back\n",
        "        _, unsort_idx = sorted_idx.sort()\n",
        "        unsorted_weights = weights[unsort_idx]\n",
        "        return robust_loss, unsorted_weights\n",
        "\n",
        "    def compute_group_avg(self, losses, group_idx):\n",
        "        # compute observed counts and mean loss for each group\n",
        "        group_map = (group_idx == torch.arange(self.n_groups).unsqueeze(1).long().to(device)).float()\n",
        "        # print(group_map.shape)\n",
        "        group_count = group_map.sum(1)\n",
        "        # print(group_count.shape)\n",
        "        group_denom = group_count + (group_count==0).float() # avoid nans\n",
        "        # print(\"group_denom.shape\",group_denom.shape)\n",
        "        # print(losses.view(-1).shape)\n",
        "        group_loss = (group_map @ losses.view(-1))/group_denom\n",
        "        return group_loss, group_count\n",
        "\n",
        "    def update_exp_avg_loss(self, group_loss, group_count):\n",
        "        prev_weights = (1 - self.gamma*(group_count>0).float()) * (self.exp_avg_initialized>0).float()\n",
        "        curr_weights = 1 - prev_weights\n",
        "        self.exp_avg_loss = self.exp_avg_loss * prev_weights + group_loss*curr_weights\n",
        "        self.exp_avg_initialized = (self.exp_avg_initialized>0) + (group_count>0)\n",
        "\n",
        "    def reset_stats(self):\n",
        "        self.processed_data_counts = torch.zeros(self.n_groups).to(device)\n",
        "        self.update_data_counts = torch.zeros(self.n_groups).to(device)\n",
        "        self.update_batch_counts = torch.zeros(self.n_groups).to(device)\n",
        "        self.avg_group_loss = torch.zeros(self.n_groups).to(device)\n",
        "        self.avg_group_acc = torch.zeros(self.n_groups).to(device)\n",
        "        self.avg_per_sample_loss = 0.\n",
        "        self.avg_actual_loss = 0.\n",
        "        self.avg_acc = 0.\n",
        "        self.batch_count = 0.\n",
        "\n",
        "    def update_stats(self, actual_loss, group_loss, group_acc, group_count, weights=None):\n",
        "        # avg group loss\n",
        "        denom = self.processed_data_counts + group_count\n",
        "        denom += (denom==0).float()\n",
        "        prev_weight = self.processed_data_counts/denom\n",
        "        curr_weight = group_count/denom\n",
        "        self.avg_group_loss = prev_weight*self.avg_group_loss + curr_weight*group_loss\n",
        "\n",
        "        # avg group acc\n",
        "        self.avg_group_acc = prev_weight*self.avg_group_acc + curr_weight*group_acc\n",
        "\n",
        "        # batch-wise average actual loss\n",
        "        denom = self.batch_count + 1\n",
        "        self.avg_actual_loss = (self.batch_count/denom)*self.avg_actual_loss + (1/denom)*actual_loss\n",
        "\n",
        "        # counts\n",
        "        self.processed_data_counts += group_count\n",
        "        if self.is_robust:\n",
        "            self.update_data_counts += group_count*((weights>0).float())\n",
        "            self.update_batch_counts += ((group_count*weights)>0).float()\n",
        "        else:\n",
        "            self.update_data_counts += group_count\n",
        "            self.update_batch_counts += (group_count>0).float()\n",
        "        self.batch_count+=1\n",
        "\n",
        "        # avg per-sample quantities\n",
        "        group_frac = self.processed_data_counts/(self.processed_data_counts.sum())\n",
        "        self.avg_per_sample_loss = group_frac @ self.avg_group_loss\n",
        "        self.avg_acc = group_frac @ self.avg_group_acc\n",
        "\n",
        "    def get_model_stats(self, model, args, stats_dict):\n",
        "        model_norm_sq = 0.\n",
        "        for param in model.parameters():\n",
        "            model_norm_sq += torch.norm(param) ** 2\n",
        "        stats_dict['model_norm_sq'] = model_norm_sq.item()\n",
        "        stats_dict['reg_loss'] = args.weight_decay / 2 * model_norm_sq.item()\n",
        "        return stats_dict\n",
        "\n",
        "    def get_stats(self, model=None, args=None):\n",
        "        stats_dict = {}\n",
        "        for idx in range(self.n_groups):\n",
        "            stats_dict[f'avg_loss_group:{idx}'] = self.avg_group_loss[idx].item()\n",
        "            stats_dict[f'exp_avg_loss_group:{idx}'] = self.exp_avg_loss[idx].item()\n",
        "            stats_dict[f'avg_acc_group:{idx}'] = self.avg_group_acc[idx].item()\n",
        "            stats_dict[f'processed_data_count_group:{idx}'] = self.processed_data_counts[idx].item()\n",
        "            stats_dict[f'update_data_count_group:{idx}'] = self.update_data_counts[idx].item()\n",
        "            stats_dict[f'update_batch_count_group:{idx}'] = self.update_batch_counts[idx].item()\n",
        "\n",
        "        stats_dict['avg_actual_loss'] = self.avg_actual_loss.item()\n",
        "        stats_dict['avg_per_sample_loss'] = self.avg_per_sample_loss.item()\n",
        "        stats_dict['avg_acc'] = self.avg_acc.item()\n",
        "\n",
        "        # Model stats\n",
        "        if model is not None:\n",
        "            assert args is not None\n",
        "            stats_dict = self.get_model_stats(model, args, stats_dict)\n",
        "\n",
        "        return stats_dict\n",
        "\n",
        "    def log_stats(self, logger, is_training):\n",
        "        if logger is None:\n",
        "            return\n",
        "\n",
        "        logger.write(f'Average incurred loss: {self.avg_per_sample_loss.item():.3f}  \\n')\n",
        "        logger.write(f'Average sample loss: {self.avg_actual_loss.item():.3f}  \\n')\n",
        "        logger.write(f'Average acc: {self.avg_acc.item():.3f}  \\n')\n",
        "        for group_idx in range(self.n_groups):\n",
        "            logger.write(\n",
        "                f'  {self.group_str(group_idx)}  '\n",
        "                f'[n = {int(self.processed_data_counts[group_idx])}]:\\t'\n",
        "                f'loss = {self.avg_group_loss[group_idx]:.3f}  '\n",
        "                f'exp loss = {self.exp_avg_loss[group_idx]:.3f}  '\n",
        "                f'adjusted loss = {self.exp_avg_loss[group_idx] + self.adj[group_idx]/torch.sqrt(self.group_counts)[group_idx]:.3f}  '\n",
        "                f'adv prob = {self.adv_probs[group_idx]:3f}   '\n",
        "                f'acc = {self.avg_group_acc[group_idx]:.3f}\\n')\n",
        "        logger.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PNF88E6TNj_u"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'robust': True,\n",
        "    'alpha': 0.02,\n",
        "    'gamma': 0.1,\n",
        "    'adjustments': None,\n",
        "    'robust_step_size': 0.01,\n",
        "    'use_normalized_loss': False,\n",
        "    'btl':False,\n",
        "    'minimum_variational_weight': 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2lIPNXOwkE5A"
      },
      "outputs": [],
      "source": [
        "# Calculate epoch time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sampling functions"
      ],
      "metadata": {
        "id": "Go7y_9Kxl5MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_balanced(train_dataset, categories, n_samples, to_boost=['black', 'muslim', 'white'], value_boost=1.5):\n",
        "    num_classes = 2\n",
        "    class_sample_counts = train_dataset.label['y'].value_counts().values\n",
        "    total_samples = class_sample_counts.sum()\n",
        "    class_weights = {}\n",
        "    cat_counts = {}\n",
        "    cat_counts_y = {}\n",
        "    y_weights = {}\n",
        "\n",
        "    for category in categories:\n",
        "        cat_counts_y[category] = {}\n",
        "        y_weights[category] = {}\n",
        "        cat_counts_y[category][1] = np.array([train_dataset.label[train_dataset.label[category] == 1]['y'].value_counts()[0], \\\n",
        "                                              train_dataset.label[train_dataset.label[category] == 1]['y'].value_counts()[1]\n",
        "                                         ])\n",
        "        cat_counts_y[category][0] = np.array([\n",
        "                                    train_dataset.label[train_dataset.label[category] == 0]['y'].value_counts()[0], \\\n",
        "                                      train_dataset.label[train_dataset.label[category] == 0]['y'].value_counts()[1]\n",
        "                                     ])\n",
        "\n",
        "        cat_counts[category] = np.array([\n",
        "                                train_dataset.label[category].value_counts()[0], \\\n",
        "                                train_dataset.label[category].value_counts()[1]\n",
        "                               ])\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights[category] = torch.Tensor(total_samples / (num_classes * cat_counts[category]))\n",
        "        #normalize\n",
        "        y_weights[category][0] = torch.Tensor(total_samples/ (num_classes * cat_counts_y[category][0]))\n",
        "        y_weights[category][1] = torch.Tensor(total_samples/ (num_classes * cat_counts_y[category][1]))\n",
        "\n",
        "        y_weights[category][0] = torch.Tensor(y_weights[category][0]/ y_weights[category][0].sum())\n",
        "        y_weights[category][1] = torch.Tensor(y_weights[category][1]/ y_weights[category][1].sum())\n",
        "\n",
        "        # Boost lows representative categories\n",
        "        if category in to_boost:\n",
        "            class_weights[category][1] *= value_boost\n",
        "            y_weights[category][1] /= value_boost if value_boost != 0 else 1\n",
        "\n",
        "\n",
        "    # Calculate general weights\n",
        "    general_weights = torch.Tensor(total_samples / (num_classes * class_sample_counts))\n",
        "    normalized_general_weights = general_weights / general_weights.sum()\n",
        "\n",
        "    # Update the 'general' entry in class_weights\n",
        "    class_weights['general'] = normalized_general_weights\n",
        "\n",
        "    train_targets = train_dataset.label['y'].tolist()\n",
        "    train_samples_weight = [\n",
        "        torch.max(torch.Tensor([(class_weights[clase][valor] * y_weights[clase][valor][y]) for clase, valor in zip(categories, row)]))\n",
        "        for y, row in zip(train_targets, train_dataset.label[categories].to_numpy())\n",
        "    ]\n",
        "    return train_samples_weight"
      ],
      "metadata": {
        "id": "GlGpOGPt12IU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3_H9k3M-5dmH"
      },
      "outputs": [],
      "source": [
        "def create_sampler_weight_some(train_dataset, categories, n_samples, to_boost=['black', 'muslim', 'white'], value_boost=1.5):\n",
        "    num_classes = 2\n",
        "    class_sample_counts = train_dataset.label['y'].value_counts().values\n",
        "    total_samples = class_sample_counts.sum()\n",
        "    class_weights = {}\n",
        "    cat_counts = {}\n",
        "    cat_counts_y = {}\n",
        "    y_weights = {}\n",
        "\n",
        "    for category in categories:\n",
        "        cat_counts_y[category] = {}\n",
        "        y_weights[category] = {}\n",
        "        cat_counts_y[category][1] = np.array([train_dataset.label[train_dataset.label[category] == 1]['y'].value_counts()[0], \\\n",
        "                                              train_dataset.label[train_dataset.label[category] == 1]['y'].value_counts()[1]\n",
        "                                         ])\n",
        "        cat_counts_y[category][0] = np.array([\n",
        "                                    train_dataset.label[train_dataset.label[category] == 0]['y'].value_counts()[0], \\\n",
        "                                      train_dataset.label[train_dataset.label[category] == 0]['y'].value_counts()[1]\n",
        "                                     ])\n",
        "\n",
        "        cat_counts[category] = np.array([\n",
        "                                train_dataset.label[category].value_counts()[0], \\\n",
        "                                train_dataset.label[category].value_counts()[1]\n",
        "                               ])\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights[category] = torch.Tensor(total_samples / (num_classes * cat_counts[category]))\n",
        "        y_weights[category][0] = torch.Tensor(total_samples/ (num_classes * cat_counts_y[category][0]))\n",
        "        y_weights[category][1] = torch.Tensor(total_samples/ (num_classes * cat_counts_y[category][1]))\n",
        "\n",
        "        y_weights[category][0] = torch.Tensor(y_weights[category][0]/ y_weights[category][0].sum())\n",
        "        y_weights[category][1] = torch.Tensor(y_weights[category][1]/ y_weights[category][1].sum())\n",
        "\n",
        "        # Boost lows representative categories\n",
        "        if category in to_boost:\n",
        "            class_weights[category][1] *= value_boost\n",
        "            y_weights[category][1] /= value_boost if value_boost != 0 else 1\n",
        "\n",
        "    # Calculate general weights\n",
        "    general_weights = torch.Tensor(total_samples / (num_classes * class_sample_counts))\n",
        "    normalized_general_weights = general_weights / general_weights.sum()\n",
        "\n",
        "    # Update the 'general' entry in class_weights\n",
        "    class_weights['general'] = normalized_general_weights\n",
        "\n",
        "    train_targets = train_dataset.label['y'].tolist()\n",
        "\n",
        "#     Calculate final sample weights\n",
        "    train_samples_weight = [\n",
        "        torch.max(torch.Tensor([(class_weights[clase][valor] * y_weights[clase][valor][y]) for clase, valor in zip(categories, row)]))\n",
        "        for y, row in zip(train_targets, train_dataset.label[categories].to_numpy())\n",
        "    ]\n",
        "\n",
        "    train_sampler = WeightedRandomSampler(train_samples_weight, n_samples, replacement=True)\n",
        "    return train_samples_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zKaVytIO0TOB"
      },
      "outputs": [],
      "source": [
        "def create_sampler(train_dataset, categories, n_samples):\n",
        "  num_classes = 2\n",
        "  class_sample_counts = train_dataset.label['y'].value_counts().values\n",
        "  total_samples = class_sample_counts.sum()\n",
        "  class_weights = {}\n",
        "  cat_counts={}\n",
        "  cat_counts_y1 ={}\n",
        "  cat_counts_y0 ={}\n",
        "  cat_counts_y ={}\n",
        "  y_weights ={}\n",
        "  for category in categories:\n",
        "    cat_counts_y[category] = {}\n",
        "    y_weights[category] = {}\n",
        "    cat_counts_y[category][1] = train_dataset.label[train_dataset.label[category]==1]['y'].value_counts().values\n",
        "    cat_counts_y[category][0] = train_dataset.label[train_dataset.label[category]==0]['y'].value_counts().values\n",
        "    cat_counts[category] = train_dataset.label[category].value_counts().values\n",
        "    class_weights[category] = torch.Tensor(total_samples / (num_classes * cat_counts[category]))\n",
        "    y_weights[category][0] = torch.Tensor(cat_counts[category].sum() / (num_classes * cat_counts_y[category][0]))\n",
        "    y_weights[category][1] = torch.Tensor(cat_counts[category].sum() / (num_classes * cat_counts_y[category][1]))\n",
        "\n",
        "  general_weights = torch.Tensor(total_samples / (num_classes * class_sample_counts))\n",
        "  normalized_general_weights = general_weights / general_weights.sum()\n",
        "\n",
        "  # Update the 'general' entry in class_weights\n",
        "  class_weights['general'] = normalized_general_weights\n",
        "\n",
        "\n",
        "  train_targets = train_dataset.label['y'].tolist()\n",
        "  train_samples_weight = [\n",
        "      torch.max(torch.Tensor([(class_weights[clase][valor]*y_weights[clase][valor][y]) for clase, valor in zip(categories, row)]))\n",
        "      for y, row in zip(train_targets, train_dataset.label[categories].to_numpy())\n",
        "  ]\n",
        "  train_sampler = WeightedRandomSampler(train_samples_weight, n_samples, replacement=True)\n",
        "  return train_sampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_balanced(train_dataset, categories, n_samples, to_boost=['black', 'muslim', 'white'], value_boost=1.5):\n",
        "    num_classes = 2\n",
        "    class_sample_counts = train_dataset.label['y'].value_counts().values\n",
        "    total_samples = class_sample_counts.sum()\n",
        "    class_weights = {}\n",
        "    cat_counts = {}\n",
        "    cat_counts_y = {}\n",
        "    y_weights = {}\n",
        "\n",
        "    for category in categories:\n",
        "        cat_counts_y[category] = {}\n",
        "        y_weights[category] = {}\n",
        "        cat_counts_y[category][1] = np.array([train_dataset.label[train_dataset.label[category] == 1]['y'].value_counts()[0], \\\n",
        "                                              train_dataset.label[train_dataset.label[category] == 1]['y'].value_counts()[1]\n",
        "                                         ])\n",
        "        cat_counts_y[category][0] = np.array([\n",
        "                                    train_dataset.label[train_dataset.label[category] == 0]['y'].value_counts()[0], \\\n",
        "                                      train_dataset.label[train_dataset.label[category] == 0]['y'].value_counts()[1]\n",
        "                                     ])\n",
        "\n",
        "        cat_counts[category] = np.array([\n",
        "                                train_dataset.label[category].value_counts()[0], \\\n",
        "                                train_dataset.label[category].value_counts()[1]\n",
        "                               ])\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights[category] = torch.Tensor(total_samples / (num_classes * cat_counts[category]))\n",
        "\n",
        "        y_weights[category][0] = torch.Tensor(total_samples/ (num_classes * cat_counts_y[category][0]))\n",
        "        y_weights[category][1] = torch.Tensor(total_samples/ (num_classes * cat_counts_y[category][1]))\n",
        "\n",
        "        y_weights[category][0] = torch.Tensor(y_weights[category][0]/ y_weights[category][0].sum())\n",
        "        y_weights[category][1] = torch.Tensor(y_weights[category][1]/ y_weights[category][1].sum())\n",
        "\n",
        "        # Boost lows representative categories\n",
        "        if category in to_boost:\n",
        "            class_weights[category][1] *= value_boost\n",
        "            y_weights[category][1] /= value_boost if value_boost != 0 else 1\n",
        "\n",
        "    # Calculate general weights\n",
        "    general_weights = torch.Tensor(total_samples / (num_classes * class_sample_counts))\n",
        "    normalized_general_weights = general_weights / general_weights.sum()\n",
        "\n",
        "    # Update the 'general' entry in class_weights\n",
        "    class_weights['general'] = normalized_general_weights\n",
        "    train_targets = train_dataset.label['y'].tolist()\n",
        "#     Calculate final sample weights\n",
        "    train_samples_weight = [\n",
        "        torch.max(torch.Tensor([(class_weights[clase][valor] * y_weights[clase][valor][y]) for clase, valor in zip(categories, row)]))\n",
        "        for y, row in zip(train_targets, train_dataset.label[categories].to_numpy())\n",
        "    ]\n",
        "#     train_sampler = WeightedRandomSampler(train_samples_weight, n_samples, replacement=True)\n",
        "    return train_samples_weight"
      ],
      "metadata": {
        "id": "iJ41_ta4q1eA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eOkdBpdfbYUL"
      },
      "outputs": [],
      "source": [
        "def check_batch_dist(train_dataloader_balanced2, id_index = 2):\n",
        "  for i, batch in tqdm(enumerate(train_dataloader_balanced2), leave=False):\n",
        "      num_categories = len(categories)\n",
        "      idx = batch[id_index]\n",
        "      # Set up subplots\n",
        "      fig, axes = plt.subplots(nrows=num_categories//3, ncols=3, figsize=(12, 3*num_categories))\n",
        "      axes = axes.flatten()\n",
        "      labels_tensor = batch[1]\n",
        "      labels_np = labels_tensor.cpu().numpy()\n",
        "      category_labels = train_dataloader_balanced2.dataset.label.iloc[idx]\n",
        "\n",
        "\n",
        "      # print(train_dataset.label.iloc[idx].value_counts())\n",
        "      unique_classes, class_counts = np.unique(labels_np, return_counts=True)\n",
        "      print(\"Class Distribution:\")\n",
        "      for class_label, count in zip(unique_classes, class_counts):\n",
        "          print(f\"Class {class_label}: {count} samples\")\n",
        "      # Per category distribution\n",
        "      # Category Distribution\n",
        "      category_data = {}\n",
        "      for j, category in enumerate(categories):\n",
        "            category_counts = category_labels.groupby([category, 'y'])['y'].size().reset_index(name='count')\n",
        "\n",
        "            # Create a subplot\n",
        "            ax = axes[j] if num_categories > 1 else axes\n",
        "\n",
        "            # Plot the bar chart for the current category\n",
        "            sns.barplot(x=category, y='count', hue='y', data=category_counts, palette='pastel', ax=ax)\n",
        "\n",
        "            # Set labels and title for each subplot\n",
        "            ax.set_xlabel('Count')\n",
        "            ax.set_ylabel('y')\n",
        "            ax.set_title(f'Counts of y for {category}')\n",
        "\n",
        "      # Show the plot after processing the first batch\n",
        "      if i == 0:\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8jRGbmUikyme"
      },
      "outputs": [],
      "source": [
        "# Just a function to print the size of batch and elements in the batch\n",
        "def check_datadimensions(data):\n",
        "  print(len(data))\n",
        "  for i in data.shape:\n",
        "    print((data[i].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V78ExW20X6v3"
      },
      "source": [
        "### Exploring data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "i_8fV5T4WeDb"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv(os.path.join(data_dir, f'train_x.csv'))\n",
        "label_df = pd.read_csv(os.path.join(data_dir, f'train_y.csv'))\n",
        "val_x_df = pd.read_csv(os.path.join(data_dir, f'val_x.csv'))\n",
        "val_y_df = pd.read_csv(os.path.join(data_dir, f'val_y.csv'))\n",
        "test_df = pd.read_csv(os.path.join(data_dir, f'test_x.csv'))\n",
        "aug_x = pd.read_csv(os.path.join(data_dir, f'augmented_clean_x.csv'))\n",
        "aug_y = pd.read_csv(os.path.join(data_dir, f'augmented_clean_y.csv'))\n",
        "merged_df_train = pd.concat([data_df, label_df], axis =1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ4V68Fjw_Qm"
      },
      "source": [
        " Histograms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rev_len = [len(i) for i in aug_x['string']]\n",
        "pd.Series(rev_len).hist()\n",
        "plt.show()\n",
        "pd.Series(rev_len).describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "xO_-SEmeF7TL",
        "outputId": "8eec61e2-5145-44b2-e645-a80c1ed0f905"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    569438.000000\n",
              "mean        363.880979\n",
              "std         276.544812\n",
              "min           1.000000\n",
              "25%         143.000000\n",
              "50%         280.000000\n",
              "75%         532.000000\n",
              "max        1891.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hed1KhOQU0u"
      },
      "source": [
        "Explore the target variable and categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "c5Zbu_HvWzpM",
        "outputId": "ad8947f5-93ba-466b-f15f-43360f35e598"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    0  1  2  3  4\n",
              "male                0  1  0  0  0\n",
              "female              0  1  0  0  0\n",
              "LGBTQ               0  0  0  0  0\n",
              "christian           0  0  0  0  0\n",
              "muslim              0  0  0  0  0\n",
              "other_religions     0  0  0  0  0\n",
              "black               1  0  0  1  0\n",
              "white               0  0  0  1  0\n",
              "identity_any        1  1  0  1  0\n",
              "severe_toxicity     0  0  0  0  0\n",
              "obscene             0  0  0  0  0\n",
              "threat              0  0  0  0  0\n",
              "insult              0  0  1  0  1\n",
              "identity_attack     1  1  0  1  0\n",
              "sexual_explicit     0  0  0  0  0\n",
              "y                   1  1  1  1  1\n",
              "from_source_domain  1  1  1  1  1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37f3d02e-ec91-4e54-9526-291f9e89c5b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>male</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LGBTQ</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>christian</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>muslim</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other_religions</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>black</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>white</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>identity_any</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>severe_toxicity</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>obscene</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>threat</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>insult</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>identity_attack</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sexual_explicit</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>from_source_domain</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37f3d02e-ec91-4e54-9526-291f9e89c5b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-37f3d02e-ec91-4e54-9526-291f9e89c5b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-37f3d02e-ec91-4e54-9526-291f9e89c5b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55536f7e-aea4-409d-aacc-2f4c702d7bd0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55536f7e-aea4-409d-aacc-2f4c702d7bd0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55536f7e-aea4-409d-aacc-2f4c702d7bd0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "label_df.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VeVq6iaXzNZ",
        "outputId": "3deb7b92-173a-4e5f-9827-f75de2ea2567"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.886577\n",
              "1    0.113423\n",
              "Name: y, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "label_df['y'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGxWOtLDYaIP",
        "outputId": "84012aa9-b480-4807-81b5-c7e3aa6b232e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.888114\n",
              "1    0.111886\n",
              "Name: y, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "val_y_df['y'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sMKtyp2kRrj4"
      },
      "outputs": [],
      "source": [
        "def get_distribution_category(label_df):\n",
        "  # List of categories\n",
        "  categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
        "  result_df = pd.DataFrame(index=categories)\n",
        "  percentages_1_0 = []\n",
        "  percentages_1_1 = []\n",
        "  percentages_0_0 = []\n",
        "  percentages_0_1 = []\n",
        "  # Iterate through each column in the DataFrame\n",
        "  for column in label_df.columns:\n",
        "      # Check if the column corresponds to the categories of interest\n",
        "      if column in categories:\n",
        "          # Calculate the percentage of occurrences of 1 for the current column\n",
        "          percentage_1_0 = (label_df[(label_df[column] == 1) & (label_df['y'] == 0)][column].sum() / len(label_df)) * 100\n",
        "          percentage_1_1 = (label_df[(label_df[column] == 1) & (label_df['y'] == 1)][column].sum() / len(label_df)) * 100\n",
        "          percentage_0_0 = (label_df[(label_df[column] == 0 )& (label_df['y'] == 0)][column].count() / len(label_df)) * 100\n",
        "          percentage_0_1 = (label_df[(label_df[column] == 0) & (label_df['y'] == 1)][column].count() / len(label_df)) * 100\n",
        "          # Store the result in the result DataFrame\n",
        "          result_df.loc[column, 'Percentage_1_0'] = percentage_1_0\n",
        "          result_df.loc[column, 'Percentage_1_1'] = percentage_1_1\n",
        "          result_df.loc[column, 'Percentage_0_0'] = percentage_0_0\n",
        "          result_df.loc[column, 'Percentage_0_1'] = percentage_0_1\n",
        "          # Append percentages to lists\n",
        "  print(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_distribution_category(aug_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSFXcKIVYdkI",
        "outputId": "fc9ab570-8081-4276-b8c8-1149a0daf987"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Percentage_1_0  Percentage_1_1  Percentage_0_0  \\\n",
            "male                   6.416326        5.865432       50.545801   \n",
            "female                 7.383420        5.360197       49.578707   \n",
            "LGBTQ                  4.998086        5.413759       51.964042   \n",
            "christian              7.098051        5.449584       49.864077   \n",
            "muslim                 6.233866        7.508104       50.728262   \n",
            "other_religions        5.969043       16.233198       50.993084   \n",
            "black                  3.688549       20.491959       53.273579   \n",
            "white                  5.712650       11.922632       51.249478   \n",
            "\n",
            "                 Percentage_0_1  \n",
            "male                  37.172440  \n",
            "female                37.677675  \n",
            "LGBTQ                 37.624114  \n",
            "christian             37.588289  \n",
            "muslim                35.529768  \n",
            "other_religions       26.804674  \n",
            "black                 22.545914  \n",
            "white                 31.115240  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_distribution_category(label_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV_XPH1hX_-Q",
        "outputId": "d0e81b53-d754-4062-c27e-7e310168a2fa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Percentage_1_0  Percentage_1_1  Percentage_0_0  \\\n",
            "male                   9.431010        1.649209       79.226726   \n",
            "female                11.627354        1.844349       77.030382   \n",
            "LGBTQ                  2.287781        0.841889       86.369955   \n",
            "christian              9.029208        0.909165       79.628528   \n",
            "muslim                 4.025082        1.161546       84.632654   \n",
            "other_religions        2.059560        0.372810       86.598176   \n",
            "black                  2.521949        1.156342       86.135788   \n",
            "white                  4.466284        1.740275       84.191453   \n",
            "\n",
            "                 Percentage_0_1  \n",
            "male                   9.693055  \n",
            "female                 9.497915  \n",
            "LGBTQ                 10.500375  \n",
            "christian             10.433099  \n",
            "muslim                10.180718  \n",
            "other_religions       10.969454  \n",
            "black                 10.185922  \n",
            "white                  9.601989  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_distribution_category(val_y_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6bfr2gbYC8e",
        "outputId": "fee2d473-f69c-4e02-d11a-22762df2a86b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Percentage_1_0  Percentage_1_1  Percentage_0_0  \\\n",
            "male                   8.964143        1.582559       79.847278   \n",
            "female                11.332448        1.706507       77.478973   \n",
            "LGBTQ                  2.432492        0.792386       86.378929   \n",
            "christian              9.220894        0.849934       79.590527   \n",
            "muslim                 3.536963        1.133245       85.274458   \n",
            "other_religions        1.823816        0.358566       86.987605   \n",
            "black                  2.476760        1.179726       86.334661   \n",
            "white                  4.459938        1.885790       84.351483   \n",
            "\n",
            "                 Percentage_0_1  \n",
            "male                   9.606020  \n",
            "female                 9.482072  \n",
            "LGBTQ                 10.396193  \n",
            "christian             10.338645  \n",
            "muslim                10.055334  \n",
            "other_religions       10.830013  \n",
            "black                 10.008853  \n",
            "white                  9.302789  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri0amRKcWeYi"
      },
      "source": [
        "The data is unbalanced also per category, most of the data is in the category ==0 for each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAD4V2pzSyNk",
        "outputId": "4455b6e1-1c77-4d1d-8e46-a884243056b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage with zeros in all categories:  0.5981199681829332\n"
          ]
        }
      ],
      "source": [
        "mask_all_zero = (label_df[categories] == 0).all(axis=1)\n",
        "rows_with_all_zero = label_df[mask_all_zero]\n",
        "print(\"Percentage with zeros in all categories: \",rows_with_all_zero.shape[0]/label_df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "59% of the data is not related to a group"
      ],
      "metadata": {
        "id": "BAsMXfu8meww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PZWA4mP2k-R4"
      },
      "outputs": [],
      "source": [
        "rows_with_nozero = label_df[~mask_all_zero]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "RsbhV5mElELC",
        "outputId": "aa53deea-5ee8-440a-80a4-83cb63919a2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        male  female  LGBTQ  christian  muslim  other_religions  black  white  \\\n",
              "1          1       1      0          0       0                0      0      0   \n",
              "3          0       0      0          0       0                0      1      1   \n",
              "8          1       0      0          0       0                0      0      1   \n",
              "10         1       0      0          0       0                0      0      1   \n",
              "18         1       1      0          0       0                0      0      0   \n",
              "...      ...     ...    ...        ...     ...              ...    ...    ...   \n",
              "269013     1       1      1          0       0                0      0      0   \n",
              "269025     1       1      0          0       0                0      0      0   \n",
              "269028     0       1      1          1       0                0      0      0   \n",
              "269034     1       0      0          0       0                0      0      1   \n",
              "269035     1       0      1          0       0                0      0      0   \n",
              "\n",
              "        identity_any  severe_toxicity  obscene  threat  insult  \\\n",
              "1                  1                0        0       0       0   \n",
              "3                  1                0        0       0       0   \n",
              "8                  1                0        0       0       1   \n",
              "10                 1                0        0       0       0   \n",
              "18                 1                0        0       0       0   \n",
              "...              ...              ...      ...     ...     ...   \n",
              "269013             1                0        0       0       0   \n",
              "269025             1                0        0       0       0   \n",
              "269028             1                0        0       0       0   \n",
              "269034             1                0        0       0       0   \n",
              "269035             1                0        0       0       0   \n",
              "\n",
              "        identity_attack  sexual_explicit  y  from_source_domain  \n",
              "1                     1                0  1                   1  \n",
              "3                     1                0  1                   1  \n",
              "8                     0                0  1                   1  \n",
              "10                    0                0  0                   1  \n",
              "18                    1                1  1                   1  \n",
              "...                 ...              ... ..                 ...  \n",
              "269013                0                0  0                   1  \n",
              "269025                0                0  0                   1  \n",
              "269028                1                0  0                   1  \n",
              "269034                0                0  0                   1  \n",
              "269035                0                0  0                   1  \n",
              "\n",
              "[32084 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03e702a0-3f96-4f91-85e5-ae184a1b061a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>male</th>\n",
              "      <th>female</th>\n",
              "      <th>LGBTQ</th>\n",
              "      <th>christian</th>\n",
              "      <th>muslim</th>\n",
              "      <th>other_religions</th>\n",
              "      <th>black</th>\n",
              "      <th>white</th>\n",
              "      <th>identity_any</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>y</th>\n",
              "      <th>from_source_domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269013</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269025</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269028</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269034</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269035</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32084 rows × 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03e702a0-3f96-4f91-85e5-ae184a1b061a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03e702a0-3f96-4f91-85e5-ae184a1b061a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03e702a0-3f96-4f91-85e5-ae184a1b061a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb515ed1-dd94-49c2-b05f-04a55a0fa26d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb515ed1-dd94-49c2-b05f-04a55a0fa26d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb515ed1-dd94-49c2-b05f-04a55a0fa26d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "rows_with_nozero[rows_with_nozero[categories].sum(axis=1) >1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ2FJWjQlqLB"
      },
      "source": [
        "Categories are not mutually exclusive, could be male and female at the same time, black and white, male and white. Etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Siqo7r78TpcR"
      },
      "source": [
        "Almost 60% doesn't belong to any of the target groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLw1CWp-YBTH"
      },
      "source": [
        "Data is imbalance to the 0 label, we should deal with it in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgt0Ui2YH_Lv",
        "outputId": "de8b1105-2149-4433-f5da-3af3d4610843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"You must have read a different article than I did. This is clearly a family grieving deeply for two members who they believe have fallen victim to mental illness. They provided insight and opted to acknowledge both victim and perpetrator as mentally ill to provided whatever sense there may be to what happened. I didn't see any back patting, any demands the boy go free. In fact they openly dispute his abuse claims and only say that his own mental illness and his mothers were a factor and explain a family effort predating the crime to help an ill member, including what is clearly now acknowledged as a flawed judgment call to send her youngest son to aide her. Perhaps you should re-read. What would you have them do? Call for a death penalty? Permanent incarceration with no mental health care? Pretend nothing was wrong before the crime?\"\n",
            " \"But inquiring liberals want to know what colour were her socks?  The outrage!!!\\n\\nTrudeau is known to wear stylish socks from skull and crossbones in Davos, Switzerland, argyle at APEC, and socks with maple leaf motifs at the First Ministers meeting in Ottawa-globeandmail\\n\\nOh my gosh.  I'm deflecting.  We're not here to talk about socks, were here to talk about decorum at important events.  No wait..I'm deflecting again.\\n\\nWe're to discuss this women behaving like Justin, no wait...darn.\\n\\nWe're here to discuss if Harper actually pocketed that communion wafer.\\n\\nAghhhh..so much trivial minutia to spew hate at....\\n\\nBack to kneeling on the couch.\"\n",
            " 'Continuing...... That\\'s why mercy becomes confusing to this line of thought.  It\\'s just ignoring God\\'s immutable law so that someone can feel better while they sin, right?  Never mind looking to see if they are loving, or trying to love, or trying to pray despite being told that the law says you are evil from the jump. Never mind whether gay or divorced people actually succeed in bettering the lives of those around them.  Never mind whether women are truly leaders and guides, true confectors of the Body of Christ.  And never mind the mercy shown me by a God who knows my daily sinfulness.  All of that pales before immutable law and the need to tell others that \"God can\\'t love you like that. God can only love you in the way I tell you, because God has revealed that (immutably) to me.\"\\nWhen the law excludes people from a church that names itself Universal, I\\'ll question it in faith. When (always sinful) people love, I\\'ll uphold that.  Because I\\'ve been taught to be Catholic.'\n",
            " \"My suspicion is that he has a few trusted advisors, lay and priests, to help him keep up with the many issues he feels compelled to rule on.   The world is big, wide, complex and bishops don't have personal experience in looking at what parents would look for in groups that can help their kids.  Unfortunately, those chosen advisors are not also chosen by all the lay people to present different views to the bishop.  So, if he choses who to listen to, and doesn't seek widely, yes, he is apt to get led around by the nose.\\n\\nI don't mean to imply that he is not responsible for the decisions he ends up making or to excuse his poor decisions or to say that those who advise him are the bad guys.  He is responsible.  \\n\\nLike you, I suspect pelvic issues, especially when it comes to women, are a primary concern.  After all, what does his training and experience lead him to expect of women?\"\n",
            " \"You're right, we should. That's a good way to distinguish him from 99.9% of the other immigrants who are not radical Islamic terrorists.\"\n",
            " \"It is not a morality that I plucked out of thin air, it's a morality that is based on the words of sacred scripture. If you don't agree with scripture take it up with God or become a Buddhist or something. It is up to us whether we choose to conform to God's will or if we prefer to have God conform to our will. People who are active, unrepentant homosexuals should not bother to call themselves Catholic as they are going against the doctrine of God's Church. This is the truth.\"\n",
            " \"Yeah, except they forgot to pick an aboriginal anchor, but I'm sure Justin Trudeau will be apologizing for this before the end of the week.  Wanna place a bet?  Also, I see no black anchors, so what's Black Lives Matter going to say about this?\"\n",
            " 'Both do not \"incite\" violence. The white supremacists are violent in it\\'s racist words and actions and the other group (BLM, antifa, etc.) is in defense of it\\'s people against the first group. No equivalency there....']\n"
          ]
        }
      ],
      "source": [
        "sampled_rows = pd.DataFrame()\n",
        "for category in categories:\n",
        "    category_rows = merged_df_train[merged_df_train[category] == 1]\n",
        "    if not category_rows.empty:\n",
        "        sampled_row = category_rows.sample(1)\n",
        "        sampled_rows = pd.concat([sampled_rows, sampled_row])\n",
        "\n",
        "# Reset the index to remove the groupby structure\n",
        "sampled_rows = sampled_rows.reset_index(drop=True)\n",
        "\n",
        "# Display the sampled rows\n",
        "print(sampled_rows['string'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary"
      ],
      "metadata": {
        "id": "2aRsmzuEm0RP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8B7OIyUh09W"
      },
      "source": [
        "Create the tokenizer from different tokenizers,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Wqtas7lsSNnk"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer.tokenize(text)\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenize_fn = 'basic_english', lower = True, max_length = None):\n",
        "\n",
        "        self.tokenize_fn = torchtext.data.utils.get_tokenizer(tokenize_fn)\n",
        "        self.lower = lower\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def tokenize(self, s):\n",
        "\n",
        "        tokens = self.tokenize_fn(s)\n",
        "\n",
        "        if self.lower:\n",
        "            tokens = [token.lower() for token in tokens]\n",
        "\n",
        "        if self.max_length is not None:\n",
        "            tokens = tokens[:self.max_length]\n",
        "\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0z_pvwiGbm"
      },
      "source": [
        "Define tokenizer from spacy and max lenght of 25000, this is the number of different tokens or different words in the data. Lets use 25000 which is recommended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Zc-FHK6PSP-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9195bb3-1e7e-4e83-e7fd-f9decc8be5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "max_length=40000 # Before 5000\n",
        "tokenizer = Tokenizer('spacy', max_length=max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN7qKzZqiQpK"
      },
      "source": [
        "Testing the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "g62cr6irSaV9"
      },
      "outputs": [],
      "source": [
        "s = 'ham but don\\'t not and'\n",
        "tokens =tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHFvIjM8iZul"
      },
      "source": [
        "Adding special tokens to the vocabulary and function to create a vocabulary from the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_x = pd.concat([data_df,val_x_df ])\n",
        "merged_x['string'] =merged_x['string'].astype(str)\n",
        "merged_x[\"string\"] = merged_x[\"string\"].str.lower()\n",
        "merged_x[\"string\"] = merged_x[\"string\"].str.replace(\"\\xa0\", \" \", regex=False).str.split().str.join(\" \")"
      ],
      "metadata": {
        "id": "lSo_J47k9CPR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6yIiA0FcSf14"
      },
      "outputs": [],
      "source": [
        "specials = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "def build_vocab_from_data(data, tokenizer, **vocab_kwarg):\n",
        "\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(data), specials=specials, max_tokens = max_length)\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m54DXxK5ijjd"
      },
      "source": [
        "Create a vocabulary from our train dataframe using the tokenizer setup before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "g0wisndjSjIu"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab_from_data(merged_x['string'], tokenizer, max_length = max_length)\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0AHEIt_aeAq"
      },
      "source": [
        "##### Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekpyrRcwiqqw"
      },
      "source": [
        "Using an already pretrained embedding helps the model to get sense from each token. Here I am using Glove embedding with 100 embedding dimensions. This will be loaded in the embedding layer of the Conv NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmJfWkP3i_Rt"
      },
      "source": [
        "For more info about this method:\n",
        "https://coderzcolumn.com/tutorials/artificial-intelligence/how-to-use-glove-embeddings-with-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmvyEovCZFlM"
      },
      "source": [
        "Get Glove embedding 100 dimensions and get vecs from actual vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaN7R0jFRnQx",
        "outputId": "d06b42a3-b717-4e93-8bb4-418b798334f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.42B.300d.zip: 1.88GB [05:53, 5.32MB/s]                            \n",
            "100%|█████████▉| 1917493/1917494 [06:04<00:00, 5267.79it/s]\n"
          ]
        }
      ],
      "source": [
        "glove = torchtext.vocab.GloVe(name='42B', dim=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59vCZa7HjFE2"
      },
      "source": [
        "Getting the embedding weights for the vocabulary we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C5aw8h9UoXR",
        "outputId": "7b0eb4e5-625b-434d-9378-81aa49017491"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        ...,\n",
              "        [-0.5394, -0.1053,  0.0788,  ..., -0.0221, -0.4653,  0.0616],\n",
              "        [ 0.3356, -0.0232, -0.3370,  ..., -0.6095, -0.5413,  0.0662],\n",
              "        [-0.3214,  0.0988, -0.2670,  ..., -0.6317, -0.5616, -0.8752]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "pretrained_embeddings = glove.get_vecs_by_tokens(vocab.get_itos(), lower_case_backup=True)\n",
        "pretrained_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6C7gYnBjKYt"
      },
      "source": [
        "Check the shape if it coinicides with the vocab size and embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx0yvAM8ZAjR",
        "outputId": "4d7a0b28-cfcd-4ff8-cfef-b6bd38113a4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40000, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "pretrained_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze0E--oVj1Q2"
      },
      "source": [
        "This is the vectorizer pipeline, so first tokenize the string, then get the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "c4wcPT5MU3vZ"
      },
      "outputs": [],
      "source": [
        "# text_pipeline = lambda x: pretrained_embeddings[vocab(tokenizer.tokenize(x))]\n",
        "text_pipeline = lambda x: vocab(tokenizer.tokenize(x))\n",
        "# Another pipeline for the LSTM\n",
        "text_pipeline_lstm = lambda x: vocab(tokenizer.tokenize(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HD-r-X9j8UZ"
      },
      "source": [
        "Testing the vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVW-GYRWavQX",
        "outputId": "671bc118-a515-4a21-c622-0de015bc1ae4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 73, 19, 17, 6, 15, 157, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "text_pipeline('I how are you, I am Javier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTk3vTnoTesR",
        "outputId": "8687adbf-833f-49b1-9b0f-0fc4f6556526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in vocab: 40000\n"
          ]
        }
      ],
      "source": [
        "print(f'Unique words in vocab: {len(vocab)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiQTGDbh1kiQ"
      },
      "source": [
        "#### Collate batch function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmgSDtWij_q1"
      },
      "source": [
        "This function is used by the data loader to prepare the data per batch. Necessary to apply some transformations per batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "w_Ie0GTg_lqP"
      },
      "outputs": [],
      "source": [
        "class Collator:\n",
        "    def __init__(self, pad_idx):\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def collate(self, batch):\n",
        "        text, labelsl, idxs = zip(*batch)\n",
        "        labels = torch.tensor(labelsl, dtype=torch.int64)\n",
        "        # print(text.dtype)\n",
        "        text = nn.utils.rnn.pad_sequence(text, padding_value=self.pad_idx, batch_first=True)\n",
        "        idx_list = torch.tensor(idxs, dtype=torch.int64)\n",
        "        return text, labels, idx_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kHJqgUhfgFmx"
      },
      "outputs": [],
      "source": [
        "class CollatorLSTM:\n",
        "    def __init__(self, pad_idx):\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def collate(self, batch):\n",
        "        text, labelsl, idxs, g = zip(*batch)\n",
        "        labels = torch.tensor(labelsl, dtype=torch.int64)\n",
        "        # print(text.dtype)\n",
        "        text = nn.utils.rnn.pad_sequence(text, padding_value=self.pad_idx, batch_first=True)\n",
        "        idx_list = torch.tensor(idxs, dtype=torch.int64)\n",
        "        g_list = torch.tensor(g, dtype=torch.int64)\n",
        "        return text, labels, idx_list, g_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10x_kJHd-Ztp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e3-Shj7g-ce"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KbpipTRhJhe",
        "outputId": "3d8fbce0-b135-4ddc-8265-a4cd2c1ebec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.11.17)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "axwF7WdthD85"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "\n",
        "from nlpaug.util import Action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sacremoses --quiet"
      ],
      "metadata": {
        "id": "CcCDjbq3ZIXO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "V69P9p98kdDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a07ed3-dd92-4516-bbbf-52fd329c5a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', aug_p=0.3, model_type='bert', device=device)\n",
        "aug_syn = naw.SynonymAug(aug_src='wordnet', lang='eng')\n",
        "aug_rnd = nac.RandomCharAug(action=\"insert\")\n",
        "def create_augmented(text, n=1):\n",
        "  augmented_text = aug.augment(text, n)\n",
        "  augmented_text = aug_syn.augment(text)\n",
        "  augmented_text = aug_rnd.augment(text)\n",
        "  # augmented_text = back_translation_aug.augment(text)\n",
        "  return augmented_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuS6w_m6idQX",
        "outputId": "65daf7f2-4248-4d65-d48d-41775d63dd78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "I always find the Star Wars/Star Trek debate funny because they seem so different to me that I have a hard time understanding how the two can be compared. Star Trek was created as a series of books and an episodic TV show, meaning that it was meant to have discreet stories told and wrapped up in a short time. Star Wars, on the other hand, was written as a series of movies with a far-reaching storyline. Star Trek is meant to be set in our own future, complete with our history and built upon our understanding of the real universe. Star Wars, however, exists \"a long time ago, in a galaxy far, far away.\" That opens SW up to potentially involve elements that would be too fantastical in a universe based on our own, no matter how futuristic. There are many other reasons, but fundamentally, comparing SW and ST does not seem like an appropriate comparison. (Also, if you insist on comparing them, it would probably be best to be done by someone with a reasonable degree of knowledge of both properties.)\n",
            "Augmented Text:\n",
            "I always find the Star Wars / Star Trek debate funny beYcuauBse thZe2y seem so different to me that I have a hard time understanding how the two can be compared. Star Trek was created as a series of books and an episodic TV show, meaning that it was meant to have discreet stories told and wrapped up in a short time. Star Wars, on the other hand, was written as a series of movies with a far - reaching story#liVn&e. Star Trek is meant to be set in our own future, complete with our history and Rbiuilt upfoLn our understanding of the real universe. Star Wars, however, exists \" a long time ago, in a galaxy far, far away. \" That opens SW up to pot4ent$iaKllAy iKnvolljve elements that would be too fantastical in a universe based on our own, no 9mattcer how futuristic. There are many other reasons, but fundamentally, comparing SW and ST does not seem like an appropriate comparison. (Also, if you gi4nsist on comparing tYhSem, it would probably be best to be done by someone with a reasonable degree of knowledge of both properties. )\n"
          ]
        }
      ],
      "source": [
        "text = 'I always find the Star Wars/Star Trek debate funny because they seem so different to me that I have a hard time understanding how the two can be compared. Star Trek was created as a series of books and an episodic TV show, meaning that it was meant to have discreet stories told and wrapped up in a short time. Star Wars, on the other hand, was written as a series of movies with a far-reaching storyline. Star Trek is meant to be set in our own future, complete with our history and built upon our understanding of the real universe. Star Wars, however, exists \"a long time ago, in a galaxy far, far away.\" That opens SW up to potentially involve elements that would be too fantastical in a universe based on our own, no matter how futuristic. There are many other reasons, but fundamentally, comparing SW and ST does not seem like an appropriate comparison. (Also, if you insist on comparing them, it would probably be best to be done by someone with a reasonable degree of knowledge of both properties.)'\n",
        "augmented_text = create_augmented(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz5o9pFti_38"
      },
      "source": [
        "#### Generate augmented rows in the train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we generated some augmented data to balance the categories."
      ],
      "metadata": {
        "id": "HNGBmSQqngy8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "njGnhrBxjNgx"
      },
      "outputs": [],
      "source": [
        "def create_new_data(train_dataset, n_samples):\n",
        "  num_classes = 2\n",
        "  class_sample_counts = train_dataset['y'].value_counts().values\n",
        "  total_samples = class_sample_counts.sum()\n",
        "  class_weights = {}\n",
        "  cat_counts={}\n",
        "  cat_counts_y1 ={}\n",
        "  cat_counts_y0 ={}\n",
        "  cat_counts_y ={}\n",
        "  y_weights ={}\n",
        "  to_create ={}\n",
        "  label_df = train_dataset\n",
        "  categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
        "  augmented_data = pd.DataFrame(columns=label_df.columns)\n",
        "  for category in categories:\n",
        "    cat_counts_y[category] = {}\n",
        "    y_weights[category] = {}\n",
        "    cat_counts_y[category][1] = train_dataset[(train_dataset[category]==1)]['y'].value_counts().values\n",
        "    cat_counts[category] = train_dataset[train_dataset[category]==1].shape[0]\n",
        "\n",
        "  for category in categories:\n",
        "    to_create[category] = int(max(cat_counts.values()) - cat_counts[category])\n",
        "\n",
        "  print(to_create)\n",
        "  return cat_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZlu6SEb0S-x"
      },
      "source": [
        "#### Create csv from augmented data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tSGq0-S7qDE-"
      },
      "outputs": [],
      "source": [
        "if(note_book_config['generate_augmented']):\n",
        "  # First we get how many records to create per category group/\n",
        "  data_to_create = create_new_data(aug_y,2)\n",
        "  to_create = ['black','other_religions','white']\n",
        "  augmented_data = pd.DataFrame(columns=merged_df_train.columns)\n",
        "  # Now, we use the function create_augmented to create new rows per category.\n",
        "  for cat in to_create:\n",
        "    data_to_copy = merged_df_train[merged_df_train[cat] == 1]\n",
        "    # Copy more y data == 1\n",
        "    data_to_copy = data_to_copy[data_to_copy['y']==1]\n",
        "    for n in tqdm(range(0, data_to_create[cat], 100)):\n",
        "      underrepresented_samples = data_to_copy.sample(100, replace=True)\n",
        "      underrepresented_samples['string'] = underrepresented_samples['string'].apply(create_augmented)\n",
        "      augmented_data = pd.concat([augmented_data, underrepresented_samples])\n",
        "      augmented_data.to_csv(data_dir+ f'/augmented/{cat}.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate and save the generated data"
      ],
      "metadata": {
        "id": "V9BdarjFoDUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kvLqb0ltyScf"
      },
      "outputs": [],
      "source": [
        "if(note_book_config['generate_augmented']):\n",
        "  dfs = []\n",
        "  data_df = pd.read_csv(os.path.join(data_dir, f'train_x.csv'),index_col=0)\n",
        "  label_df = pd.read_csv(os.path.join(data_dir, f'train_y.csv'))\n",
        "  csv_files = [file for file in os.listdir(drive_dir+'/kaggle_data/augmented/last/') if file.endswith('.csv')]\n",
        "  for csv_file in csv_files:\n",
        "      file_path = os.path.join(drive_dir+'/kaggle_data/augmented/last/', csv_file)\n",
        "      df = pd.read_csv(file_path, index_col=1, usecols=lambda x: x not in [0])\n",
        "      dfs.append(df)\n",
        "\n",
        "  concatenated_df = pd.concat(dfs, ignore_index=True)\n",
        "  concatenated_df.reset_index(drop=True, inplace=True)\n",
        "  concatenated_df.iloc[:, 1] = concatenated_df.iloc[:, 1].astype(str)\n",
        "  concatenated_df.iloc[:, 1] = concatenated_df.iloc[:, 1].apply(denoise_text)\n",
        "  #Concatenate both\n",
        "  aug_df = pd.read_csv(os.path.join(data_dir, f'augmented_x.csv'),index_col=0)\n",
        "  aug_label_df = pd.read_csv(os.path.join(data_dir, f'augmented_y.csv'))\n",
        "  df_x_augm = pd.concat([concatenated_df[aug_df.columns], aug_df])\n",
        "  df_y_augm = pd.concat([concatenated_df[aug_label_df.columns], aug_label_df])\n",
        "\n",
        "  df_x_augm.to_csv(drive_dir+'/kaggle_data/aug_x.csv', index=True)\n",
        "  df_y_augm.to_csv(drive_dir+'/kaggle_data/aug_y.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkeHlrLU0ksm"
      },
      "source": [
        "## ConvNN Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXX32-BNkIrI"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7uGn2lQUdTNa"
      },
      "outputs": [],
      "source": [
        "class JaviDataset(Dataset):\n",
        "    def __init__(self, data_dir, mode, vectorizer=None):\n",
        "        super(JaviDataset, self).__init__()\n",
        "        assert mode in ['train', 'val', 'test', 'augmented']\n",
        "        self.mode = mode\n",
        "\n",
        "        # load the data\n",
        "        self.data = pd.read_csv(os.path.join(data_dir, f'{mode}_x.csv'), index_col=0)\n",
        "\n",
        "        #clean data\n",
        "        self.data.iloc[:, 0] = self.data.iloc[:, 0].apply(denoise_text)\n",
        "\n",
        "        # load the labels if not the test set\n",
        "        if self.mode != 'test':\n",
        "            self.label = pd.read_csv(os.path.join(data_dir, f'{mode}_y.csv'))\n",
        "\n",
        "        # train the vectorizer if train set\n",
        "        if self.mode in ['train','augmented']:\n",
        "            self.vectorizer = vectorizer\n",
        "            # self.vectorizer.fit(self.data.values.flatten().tolist())\n",
        "        # otherwise use the vectorizer given as arguments (which was trained on the train set)\n",
        "        else:\n",
        "            self.vectorizer = vectorizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data.iloc[idx, 0]\n",
        "        x = self.vectorizer(x)\n",
        "        # print(x)\n",
        "        x = torch.tensor(x).int()\n",
        "        if self.mode == 'test':\n",
        "            return x, idx, idx\n",
        "        else:\n",
        "            y = torch.tensor([self.label.iloc[idx]['y']], dtype=torch.int64)\n",
        "            return x, y, idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "l2zJjeiYf0fi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U3GElsDhU5i"
      },
      "source": [
        "Convolutional neural networw with an embedding layer.\n",
        "Different filters and filter sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "nvL15Nx5WEQ7"
      },
      "outputs": [],
      "source": [
        "class JaviClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n",
        "                 dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        self.embeddingBag = nn.EmbeddingBag(vocab_size, embedding_dim, padding_idx = pad_idx,sparse=True,)\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1,\n",
        "                                              out_channels = n_filters,\n",
        "                                              kernel_size = (fs, embedding_dim))\n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        ret = self.fc(cat)\n",
        "        # x = F.sigmoid(cat)\n",
        "        return F.sigmoid(ret)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y_T9s3QkxjU"
      },
      "source": [
        "Here testing the conv NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "J_8X2NPhZKRj"
      },
      "outputs": [],
      "source": [
        "def train_model_conv(model, optimizer, criterion, dataloader):\n",
        "    \"\"\"\n",
        "        Train a model for one epoch.\n",
        "        arguments:\n",
        "            model [torch.nn.Module]: model to evaluate\n",
        "            oprimizer [torch.optim]: optimizer used for training\n",
        "            criterion [torch.nn.modules.loss]: desired loss to compute\n",
        "            dataloader [torch.utils.data.DataLoader]: dataloader used for training\n",
        "        returns:\n",
        "            dataset_loss [float]: computed loss on the dataset\n",
        "            dataset_metric [float]: computed metric on the dataset\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    losses, predictions, indices = [], [], []\n",
        "    for x, y, idx in tqdm(dataloader, leave=False):\n",
        "        x, y = x.to(device).int(), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        if(x.shape[1] > max(model.filter_sizes)):\n",
        "          pred = model(x)\n",
        "          loss = criterion(pred.squeeze(1), y.float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.extend([loss.item()] * len(y))\n",
        "          predictions.extend(pred.detach().squeeze().tolist())\n",
        "          indices.extend(idx.tolist())\n",
        "\n",
        "\n",
        "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "    dataset_loss = np.mean(losses)\n",
        "    dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy())\n",
        "    return dataset_loss, dataset_metric, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw-f8IlLON1G"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "p4g2oH0RQwBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0c3075-348d-4b1b-9b12-86be7c8d7945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "train_dataset2 = JaviDataset(data_dir, 'train', text_pipeline)\n",
        "val_dataset2 = JaviDataset(data_dir, 'val', train_dataset2.vectorizer )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4KVDNkCk0rM"
      },
      "source": [
        "Create a collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ffQqhD8lE3a0"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = vocab.get_stoi()[\"<unk>\"]\n",
        "collator = Collator(PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "WS3At7V6RuPx"
      },
      "outputs": [],
      "source": [
        "# train_dataloader_bal_pro = DataLoader(train_dataset2, batch_size=32,,collate_fn=collator.collate)\n",
        "train_dataloader_bal_pro = DataLoader(train_dataset2, batch_size=32, shuffle=True, collate_fn=collator.collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "FHa7VO2G1Y7K"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = model_params['convnn0']['INPUT_DIM']\n",
        "EMBEDDING_DIM = model_params['convnn0']['EMBEDDING_DIM']\n",
        "N_FILTERS = model_params['convnn0']['N_FILTERS']\n",
        "FILTER_SIZES = model_params['convnn0']['FILTER_SIZES']\n",
        "OUTPUT_DIM = model_params['convnn0']['OUTPUT_DIM']\n",
        "DROPOUT = model_params['convnn0']['DROPOUT']\n",
        "LR = model_params['convnn0']['LR']\n",
        "\n",
        "criterion = nn.BCELoss().to(device)\n",
        "\n",
        "model = JaviClassifier(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX).to(device)\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO7wzKpCZnhy"
      },
      "source": [
        "Preload embeddings from the Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guL2F1KzaL65",
        "outputId": "092c404f-c52e-4b3d-b442-7a444bfb47c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "model.embedding.load_state_dict({'weight': pretrained_embeddings})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQFaG0rjt-B"
      },
      "source": [
        "initialize specials with random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FtYn-fPjiT7c"
      },
      "outputs": [],
      "source": [
        "PAD_IDX_s = [vocab.get_stoi()[special] for special in specials]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "aheKNkw1iOh9"
      },
      "outputs": [],
      "source": [
        "pretrained_embeddings[PAD_IDX_s] = torch.randn((EMBEDDING_DIM, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "k9lX0dfFMbIO"
      },
      "outputs": [],
      "source": [
        "val_dataloader_pro = DataLoader(val_dataset2, batch_size=32, collate_fn=collator.collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcc3iWeik9Zr"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "S3p57y6Hpve_"
      },
      "outputs": [],
      "source": [
        "PATIENCE = 5\n",
        "MODEL_FILE_NAME = 'kaggle_dataconvnn_2-model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBzwPnZgj-mS"
      },
      "outputs": [],
      "source": [
        "loss_history = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_metric, losses = train_model_conv(model, optimizer, criterion, train_dataloader_bal_pro)\n",
        "    mlp_val_loss, mlp_val_metric, pred_df = evaluate_model(model, val_dataloader_pro, criterion, False)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | WG Acc: {train_metric*100:.2f}%')\n",
        "    print(f'\\tVal Loss: {mlp_val_loss:.3f} | WG Acc: {mlp_val_metric*100:.2f}%')\n",
        "    if len(loss_history) == 0 or mlp_val_loss < min(loss_history):\n",
        "        no_improvement = 0\n",
        "        torch.save(model.state_dict(),  os.path.join(drive_dir, MODEL_FILE_NAME))\n",
        "    else:\n",
        "        no_improvement += 1\n",
        "    # if no_improvement >= PATIENCE:\n",
        "    #     print(\"No improvement on development set. Finish training.\")\n",
        "    #     break\n",
        "    loss_history.append(mlp_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwtbPjJcleuy"
      },
      "source": [
        "Save the model state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFvb9OuK8DV6"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), data_dir+'convnn_2-model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnX44rbHOIIH"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "01db701dc76340d9b69948b350bc8776",
            "b3442a2c32a14eefa1080d265053b0f5",
            "5f911f2d04ea4640a57be12511a75eeb",
            "34cacc52efb2418e88ada1cb064ffe48",
            "85d7e461f17e45f5a16d4ee29af5d5f7",
            "51af2460702e44b4a67393159d9cf75f",
            "20bc98e5655d44359eaf33c94484481b",
            "3fa5cbc37857433c8191468d19768894",
            "283470ab74b34fe99d7527ce5d3d062b",
            "2d6f42abe847480ca96e40a4ce34a724",
            "f640c1717cd54ce88cc25dd5783de6bf"
          ]
        },
        "id": "F9G19LYnMYae",
        "outputId": "ecd918de-bede-44d6-8999-c7168ee32522"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01db701dc76340d9b69948b350bc8776",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1412 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category: male_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91     36075\n",
            "           1       0.37      0.60      0.46      4340\n",
            "\n",
            "    accuracy                           0.85     40415\n",
            "   macro avg       0.66      0.74      0.69     40415\n",
            "weighted avg       0.89      0.85      0.86     40415\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: male_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.90      4050\n",
            "           1       0.43      0.53      0.48       715\n",
            "\n",
            "    accuracy                           0.83      4765\n",
            "   macro avg       0.67      0.70      0.69      4765\n",
            "weighted avg       0.84      0.83      0.83      4765\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: female_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91     35005\n",
            "           1       0.38      0.60      0.46      4284\n",
            "\n",
            "    accuracy                           0.85     39289\n",
            "   macro avg       0.66      0.74      0.69     39289\n",
            "weighted avg       0.88      0.85      0.86     39289\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: female_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.87      0.90      5120\n",
            "           1       0.40      0.56      0.47       771\n",
            "\n",
            "    accuracy                           0.83      5891\n",
            "   macro avg       0.66      0.72      0.68      5891\n",
            "weighted avg       0.86      0.83      0.84      5891\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: LGBTQ_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91     39026\n",
            "           1       0.37      0.60      0.46      4697\n",
            "\n",
            "    accuracy                           0.85     43723\n",
            "   macro avg       0.66      0.74      0.69     43723\n",
            "weighted avg       0.89      0.85      0.86     43723\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: LGBTQ_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84      1099\n",
            "           1       0.51      0.46      0.48       358\n",
            "\n",
            "    accuracy                           0.76      1457\n",
            "   macro avg       0.67      0.66      0.66      1457\n",
            "weighted avg       0.75      0.76      0.75      1457\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: christian_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.87      0.91     35959\n",
            "           1       0.38      0.60      0.47      4671\n",
            "\n",
            "    accuracy                           0.84     40630\n",
            "   macro avg       0.66      0.74      0.69     40630\n",
            "weighted avg       0.88      0.84      0.86     40630\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: christian_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94      4166\n",
            "           1       0.36      0.42      0.39       384\n",
            "\n",
            "    accuracy                           0.89      4550\n",
            "   macro avg       0.65      0.68      0.66      4550\n",
            "weighted avg       0.90      0.89      0.89      4550\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: muslim_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91     38527\n",
            "           1       0.37      0.60      0.46      4543\n",
            "\n",
            "    accuracy                           0.85     43070\n",
            "   macro avg       0.66      0.74      0.69     43070\n",
            "weighted avg       0.89      0.85      0.86     43070\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: muslim_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85      1598\n",
            "           1       0.53      0.48      0.50       512\n",
            "\n",
            "    accuracy                           0.77      2110\n",
            "   macro avg       0.68      0.67      0.68      2110\n",
            "weighted avg       0.76      0.77      0.77      2110\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: other_religions_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91     39301\n",
            "           1       0.38      0.60      0.46      4893\n",
            "\n",
            "    accuracy                           0.85     44194\n",
            "   macro avg       0.66      0.74      0.69     44194\n",
            "weighted avg       0.88      0.85      0.86     44194\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: other_religions_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.91      0.90       824\n",
            "           1       0.47      0.40      0.43       162\n",
            "\n",
            "    accuracy                           0.83       986\n",
            "   macro avg       0.68      0.66      0.67       986\n",
            "weighted avg       0.82      0.83      0.82       986\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: black_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91     39006\n",
            "           1       0.37      0.60      0.46      4522\n",
            "\n",
            "    accuracy                           0.85     43528\n",
            "   macro avg       0.66      0.74      0.69     43528\n",
            "weighted avg       0.89      0.85      0.87     43528\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: black_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.81      0.78      1119\n",
            "           1       0.53      0.46      0.49       533\n",
            "\n",
            "    accuracy                           0.70      1652\n",
            "   macro avg       0.65      0.63      0.64      1652\n",
            "weighted avg       0.69      0.70      0.69      1652\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: white_0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.92     38110\n",
            "           1       0.36      0.61      0.45      4203\n",
            "\n",
            "    accuracy                           0.86     42313\n",
            "   macro avg       0.66      0.74      0.69     42313\n",
            "weighted avg       0.89      0.86      0.87     42313\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "Category: white_1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.80      0.79      2015\n",
            "           1       0.51      0.51      0.51       852\n",
            "\n",
            "    accuracy                           0.71      2867\n",
            "   macro avg       0.65      0.65      0.65      2867\n",
            "weighted avg       0.71      0.71      0.71      2867\n",
            "\n",
            "\n",
            "========================================\n",
            "\n",
            "MLP classifier validation loss 0.5105 WGA 0.6955\n"
          ]
        }
      ],
      "source": [
        "mlp_val_loss, mlp_val_metric, pred_df = evaluate_model(model, val_dataloader_pro, criterion, True)\n",
        "print(f'MLP classifier validation loss {mlp_val_loss:.4f} WGA {mlp_val_metric:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p3VHpmifXux"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zFQzzUvKrO9c"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_LSTM(model, dataloader, criterion, cr =False):\n",
        "    \"\"\"\n",
        "        Evaluate the model on a given dataloader.\n",
        "        argument:\n",
        "            model [torch.nn.Module]: model to evaluate\n",
        "            dataloader [torch.utils.data.DataLoader]: dataloader on which to evaluate\n",
        "            criterion [torch.nn.modules.loss]: desired loss to compute\n",
        "        returns:\n",
        "            dataset_loss [float]: computed loss on the dataset\n",
        "            dataset_metric [float]: computed metric on the dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses, predictions, indices = [], [], []\n",
        "    for x, y, idx, g in tqdm(dataloader, leave=False):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(x)\n",
        "        loss = criterion(pred.squeeze(), y.squeeze().float())\n",
        "        losses.extend([loss.item()] * len(y))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idx.tolist())\n",
        "\n",
        "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "    dataset_loss = np.mean(losses)\n",
        "    dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy(), cr)\n",
        "    return dataset_loss, dataset_metric, pred_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "pe5hd6klsgYj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "AOU1N1z2duzP"
      },
      "outputs": [],
      "source": [
        "class LSTMDataset(Dataset):\n",
        "    def __init__(self, data_dir, mode, vectorizer=None):\n",
        "        super(LSTMDataset, self).__init__()\n",
        "        assert mode in ['train', 'val', 'test', 'augmented', 'augmented_clean','combined']\n",
        "        self.mode = mode\n",
        "        self.groups = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
        "        # load the data\n",
        "        self.data = pd.read_csv(os.path.join(data_dir, f'{mode}_x.csv'), index_col=0)\n",
        "        if(mode != 'augmented_clean'):\n",
        "            #clean data\n",
        "            self.data.iloc[:, 0] = self.data.iloc[:, 0].apply(denoise_text)\n",
        "            #remove stop words\n",
        "            self.data.iloc[:, 0] = self.data.iloc[:, 0].apply(remove_stop_faster)\n",
        "        # load the labels if not the test set\n",
        "        if self.mode != 'test':\n",
        "            self.label = pd.read_csv(os.path.join(data_dir, f'{mode}_y.csv'))\n",
        "\n",
        "        # train the vectorizer if train set\n",
        "        if self.mode in ['train','augmented','augmented_clean','combined']:\n",
        "            self.vectorizer = vectorizer\n",
        "        # otherwise use the vectorizer given as arguments (which was trained on the train set)\n",
        "        else:\n",
        "            self.vectorizer = vectorizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data.iloc[idx, 0]\n",
        "        x = self.vectorizer(str(x))\n",
        "        # print(x)\n",
        "        x = torch.tensor(x).int()\n",
        "        if self.mode == 'test':\n",
        "            return x, idx, idx\n",
        "        else:\n",
        "            y = torch.tensor([self.label.iloc[idx]['y']], dtype=torch.int64)\n",
        "            # g = [1 if self.label.iloc[idx][group] == 1 else 0 for i, group in enumerate(self.groups) ]\n",
        "            g_index = next((i for i, group in enumerate(self.groups) if self.label.iloc[idx][group] == 1), None)\n",
        "\n",
        "            # Check if a match was found\n",
        "            if g_index is not None:\n",
        "                g = torch.tensor(g_index, dtype=torch.int64)\n",
        "            else:\n",
        "                # Handle the case where no match was found\n",
        "                g = torch.tensor(-1, dtype=torch.int64)  # Set to -1 or any default value\n",
        "            return x, y, idx, g\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "JGxWRFUDxQR6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "LrfcVM_qfZaY"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs, dropout=0, drop_layer =False, pad_idx=3, bi=False, **args):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.V = n_vocab\n",
        "        self.D = embed_dim\n",
        "        self.M = n_hidden\n",
        "        self.K = n_outputs\n",
        "        self.L = n_rnnlayers\n",
        "        self.dropout = dropout\n",
        "        self.droplayer = drop_layer\n",
        "        self.bi = bi\n",
        "\n",
        "        self.embed = nn.Embedding(self.V, self.D, padding_idx = pad_idx)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.D,\n",
        "            hidden_size=self.M,\n",
        "            num_layers=self.L,\n",
        "            dropout=self.dropout,\n",
        "            bidirectional = self.bi,\n",
        "            batch_first=True)\n",
        "        self.drop_layer = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.M, self.K)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, X):\n",
        "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "        c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "\n",
        "        out = self.embed(X)\n",
        "\n",
        "        # get RNN unit output\n",
        "        out, _ = self.rnn(out, (h0, c0))\n",
        "\n",
        "        # max pool\n",
        "        out, _ = torch.max(out, 1)\n",
        "\n",
        "        # we only want h(T) at the final time step\n",
        "        out = self.fc(out)\n",
        "        return F.sigmoid(out)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.L,batch_size,self.M)).to(device)\n",
        "        c0 = torch.zeros((self.L,batch_size,self.M)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model which keep track of the hidden state of last batch\n",
        "class LSTM_Track(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs, dropout=0, drop_layer =False, pad_idx=3, bi=False):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.V = n_vocab\n",
        "        self.D = embed_dim\n",
        "        self.M = n_hidden\n",
        "        self.K = n_outputs\n",
        "        self.L = n_rnnlayers\n",
        "        self.dropout = dropout\n",
        "        self.droplayer = drop_layer\n",
        "        self.bi = bi\n",
        "\n",
        "        self.embed = nn.Embedding(self.V, self.D, padding_idx = pad_idx)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.D,\n",
        "            hidden_size=self.M,\n",
        "            num_layers=self.L,\n",
        "            dropout=self.dropout,\n",
        "            bidirectional = self.bi,\n",
        "            batch_first=True)\n",
        "        self.drop_layer = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.M, self.K)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, X, hidden):\n",
        "#         h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "#         c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "        batch_size = X.size(0)\n",
        "        embeds = self.embed(X)\n",
        "        #For last batch of less than batch size\n",
        "        if(hidden[0].size(1) != embeds.size(0)):\n",
        "            h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "            c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "            hidden = (h0, c0)\n",
        "        # get RNN unit output\n",
        "\n",
        "#         print(hidden)\n",
        "        lstm_out, hidden = self.rnn(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.M)\n",
        "\n",
        "        if(self.droplayer):\n",
        "            lstm_out = self.drop_layer(lstm_out)\n",
        "\n",
        "#         # max pool\n",
        "#         out, _ = torch.max(out, 1)\n",
        "        # we only want h(T) at the final time step\n",
        "        out = self.fc(lstm_out)\n",
        "#         Sigmoid\n",
        "        sig_out = self.sig(out)\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.L,batch_size,self.M)).to(device)\n",
        "        c0 = torch.zeros((self.L,batch_size,self.M)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "N1Ug3opzfbJn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Doee420Zeevb"
      },
      "outputs": [],
      "source": [
        "def train_model_lstm(model, optimizer, criterion, dataloader):\n",
        "    \"\"\"\n",
        "        Train a model for one epoch.\n",
        "        arguments:\n",
        "            model [torch.nn.Module]: model to evaluate\n",
        "            oprimizer [torch.optim]: optimizer used for training\n",
        "            criterion [torch.nn.modules.loss]: desired loss to compute\n",
        "            dataloader [torch.utils.data.DataLoader]: dataloader used for training\n",
        "        returns:\n",
        "            dataset_loss [float]: computed loss on the dataset\n",
        "            dataset_metric [float]: computed metric on the dataset\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    losses, predictions, indices = [], [], []\n",
        "    for x, y, idx in tqdm(dataloader, leave=False):\n",
        "        x, y = x.to(device).int(), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred.squeeze(1), y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.extend([loss.item()] * len(y))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idx.tolist())\n",
        "\n",
        "\n",
        "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "    dataset_loss = np.mean(losses)\n",
        "    dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy())\n",
        "    return dataset_loss, dataset_metric, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "NPDlHIqxcdJH"
      },
      "outputs": [],
      "source": [
        "def train_model_lstm_custom(model, optimizer, criterion, dataloader, custom_loss=True):\n",
        "    \"\"\"\n",
        "        Train a model for one epoch.\n",
        "        arguments:\n",
        "            model [torch.nn.Module]: model to evaluate\n",
        "            oprimizer [torch.optim]: optimizer used for training\n",
        "            criterion [torch.nn.modules.loss]: desired loss to compute\n",
        "            dataloader [torch.utils.data.DataLoader]: dataloader used for training\n",
        "        returns:\n",
        "            dataset_loss [float]: computed loss on the dataset\n",
        "            dataset_metric [float]: computed metric on the dataset\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    losses, predictions, indices = [], [], []\n",
        "    for x, y, idx, g in tqdm(dataloader, leave=False):\n",
        "        x, y,g = x.to(device).int(), y.to(device), g.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        if(custom_loss):\n",
        "            loss = criterion.loss(pred.squeeze(1), y.float(), g, True)\n",
        "        else:\n",
        "            loss = criterion(pred.squeeze(1), y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.extend([loss.item()] * len(y))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idx.tolist())\n",
        "\n",
        "\n",
        "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "    dataset_loss = np.mean(losses)\n",
        "    dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy())\n",
        "    return dataset_loss, dataset_metric, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHn7Zv6aJ0yE"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "WdfR-izhMl4K"
      },
      "outputs": [],
      "source": [
        "BS = model_params['lstm_yj']['BS']\n",
        "N_EPOCHS = model_params['lstm_yj']['N_EPOCHS']\n",
        "MODEL_FILE_NAME = 'LSTM_DRO_.bin'\n",
        "EMBEDDING_DIM =  model_params['lstm_yj']['EMBEDDING_DIM']\n",
        "DROPOUT = model_params['lstm_yj']['DROPOUT']\n",
        "LR = model_params['lstm_yj']['LR']\n",
        "HIDDEN_DIM = model_params['lstm_yj']['HIDDEN_DIM']\n",
        "NLAYERS = model_params['lstm_yj']['N_LAYERS']\n",
        "BI = model_params['lstm_yj']['BI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "KaEhSLjmJ0ZT"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss().to(device)\n",
        "model = LSTM(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, NLAYERS, 1,DROPOUT)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "#initialize embedded layer\n",
        "model.embed.load_state_dict({'weight': pretrained_embeddings})\n",
        "pad_token = '<PAD>'\n",
        "pad_idx = vocab[pad_token]\n",
        "PAD_IDX_s = [vocab.get_stoi()[special] for special in specials]\n",
        "pretrained_embeddings[PAD_IDX_s] = torch.randn((EMBEDDING_DIM, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "cE49ylDggtxy"
      },
      "outputs": [],
      "source": [
        "collator = CollatorLSTM(pad_idx)\n",
        "collatorVal = Collator(pad_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OvMOcj5J_qj",
        "outputId": "c705aeb4-ad98-4e4c-fae4-7e01e4b8b854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-12-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "# train_dataset2 = JaviDataset(data_dir, 'train')\n",
        "# val_dataset2 = JaviDataset(data_dir, 'val', train_dataset2.vectorizer )\n",
        "# Augmented data\n",
        "train_dataset2 = LSTMDataset(data_dir, 'combined', text_pipeline_lstm)\n",
        "val_dataset2 = LSTMDataset(data_dir, 'val', train_dataset2.vectorizer )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "rAUQjxZvu8wN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming len(train_ds) is the total number of samples in your dataset\n",
        "total_samples = len(train_dataset2)\n",
        "subset_size = 10\n",
        "# Create a random subset sampler\n",
        "subset_sampler = SubsetRandomSampler(range(subset_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "dG8lE8KFKqgV"
      },
      "outputs": [],
      "source": [
        "# train_dataloader_bal = DataLoader(train_dataset2, batch_size=BS, sampler=train_sampler,collate_fn=collator.collate)\n",
        "train_dl_short = DataLoader(train_dataset2, batch_size=BS, sampler=subset_sampler,collate_fn=collator.collate)\n",
        "train_dataloader_shuffled = DataLoader(train_dataset2, batch_size=BS, shuffle=True,collate_fn=collator.collate)\n",
        "val_dataloader_pro = DataLoader(val_dataset2, batch_size=BS, collate_fn=collator.collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bAUH19ALUhB"
      },
      "outputs": [],
      "source": [
        "loss_history = []\n",
        "val_metric = []\n",
        "CLIP = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    # initialize hidden state\n",
        "    start_time = time.time()\n",
        "    #Train loop\n",
        "    # train_loss, train_metric, losses = train_model_lstm_custom(model, optimizer, criterion, train_dl_short, False)\n",
        "    train_loss, train_metric, losses = train_model_lstm_custom(model, optimizer, criterion, train_dataloader_shuffled, False)\n",
        "    #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "    optimizer.step()\n",
        "    # Validation loop\n",
        "    #mlp_val_loss, mlp_val_metric, pred_df = evaluate_model_LSTM(model, val_dataloader_pro, criterion, False)\n",
        "#     mlp_val_loss, mlp_val_metric, pred_df = evaluate_model_LSTM(model, valataloader_bal_sampled, criterion, False)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | WG Acc: {train_metric*100:.2f}%')\n",
        "    #print(f'\\tVal Loss: {mlp_val_loss:.3f} | WG Acc: {mlp_val_metric*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH1fHlY8uj87"
      },
      "source": [
        "## BERT V0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the batch size, recommended 16, 32\n",
        "BATCH_SIZE = model_params['bert0']['BATCH_SIZE']\n",
        "MAX_SEQ_LENGTH =   model_params['bert0']['MAX_SEQ_LENGTH']\n",
        "GRADIENT_ACCUMULATION_STEPS =  model_params['bert0']['GRADIENT_ACCUMULATION_STEPS']\n",
        "NUM_TRAIN_EPOCHS =  model_params['bert0']['NUM_TRAIN_EPOCHS']\n",
        "WARMUP_PROPORTION = model_params['bert0']['WARMUP_PROPORTION']\n",
        "MAX_GRAD_NORM = model_params['bert0']['MAX_GRAD_NORM']\n",
        "PATIENCE =model_params['bert0']['PATIENCE']\n",
        "MAX_LEN = model_params['bert0']['MAX_SEQ_LENGTH']\n",
        "TRAIN_BATCH_SIZE = model_params['bert0']['BATCH_SIZE']\n",
        "EPOCHS = model_params['bert0']['NUM_TRAIN_EPOCHS']\n",
        "LEARNING_RATE = model_params['bert0']['LEARNING_RATE']\n",
        "NUM_WORKERS = model_params['bert0']['NUM_WORKERS']"
      ],
      "metadata": {
        "id": "AJxAeBKruj88"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some functions"
      ],
      "metadata": {
        "id": "Rs5cZhk1uj89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_BERT( model, optimizer, criterion, dataloader, scheduler,GRADIENT_ACCUMULATION_STEPS):\n",
        "  model.train()\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  losses, predictions, indices = [], [], []\n",
        "  for step, batch in enumerate(tqdm(dataloader, desc=\"Training iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids, idxs = batch\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids.float())\n",
        "        loss = outputs[0]\n",
        "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        pred = np.argmax(outputs['logits'].detach().to('cpu'), axis=1)\n",
        "        losses.extend([loss.item()] * len(label_ids))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idxs.tolist())\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "  pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "  dataset_loss = np.mean(losses)\n",
        "  dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy())\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  return dataset_loss, dataset_metric, losses"
      ],
      "metadata": {
        "id": "ncxTAlH0uj89"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_BERT_model(model, dataloader, criterion, cr =False):\n",
        "    \"\"\"\n",
        "        Evaluate the model on a given dataloader.\n",
        "        argument:\n",
        "            model [torch.nn.Module]: model to evaluate\n",
        "            dataloader [torch.utils.data.DataLoader]: dataloader on which to evaluate\n",
        "            criterion [torch.nn.modules.loss]: desired loss to compute\n",
        "        returns:\n",
        "            dataset_loss [float]: computed loss on the dataset\n",
        "            dataset_metric [float]: computed metric on the dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses, predictions, indices = [], [], []\n",
        "    eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    for batch in tqdm(dataloader, leave=False):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, y, idx = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "          res = model(input_ids, attention_mask=input_mask,\n",
        "                                          token_type_ids=segment_ids, labels=y)\n",
        "\n",
        "        pred = np.argmax(res['logits'].to('cpu'), axis=1)\n",
        "        # loss = criterion(pred.squeeze().to(device), y.squeeze())\n",
        "        eval_loss += res['loss'].mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        losses.extend([res['loss'].mean().item()] * len(y))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idx.tolist())\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "    dataset_loss = np.mean(losses)\n",
        "    dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy(), cr)\n",
        "    return dataset_loss, dataset_metric, pred_df"
      ],
      "metadata": {
        "id": "O-MPeOFHuj89"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "d-r3K4sWuj8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "esLQLyapuj8-"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "IpwaKrdLuj8-"
      },
      "outputs": [],
      "source": [
        "BERT_MODEL ='bert-base-uncased'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "slP59Ynhuj8_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3f9dc9-2480-4a29-a4a6-2b7f57e863c3",
        "id": "CZUVcs1zuj8_"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "## BERT\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(BERT_MODEL)\n",
        "model = BertModel.from_pretrained(BERT_MODEL)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "_d80Psyzuj9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not using this by now, I was thinking on changing for a more eficient data loader thant the function get_data_loader"
      ],
      "metadata": {
        "id": "NOxsiMHfuj9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "DeCAtG0Wuj9B"
      },
      "outputs": [],
      "source": [
        "#Working on it, still not rready\n",
        "class BertDataset(Dataset):\n",
        "    def __init__(self, data_dir, mode, vectorizer=None, max_seq_length=100):\n",
        "        super(BertDataset, self).__init__()\n",
        "        assert mode in ['train', 'val', 'test', 'augmented', 'augmented_clean','combined']\n",
        "        self.mode = mode\n",
        "        self.max_seq_length = max_seq_length\n",
        "        # load the data\n",
        "        self.data = pd.read_csv(os.path.join(data_dir, f'{mode}_x.csv'), index_col=0)\n",
        "\n",
        "        # load the labels if not the test set\n",
        "        if self.mode != 'test':\n",
        "            self.label = pd.read_csv(os.path.join(data_dir, f'{mode}_y.csv'))\n",
        "\n",
        "        # train the vectorizer if train set\n",
        "        if self.mode in ['train','augmented', 'augmented_clean','combined']:\n",
        "            self.vectorizer = vectorizer\n",
        "            # self.vectorizer.fit(self.data.values.flatten().tolist())\n",
        "        # otherwise use the vectorizer given as arguments (which was trained on the train set)\n",
        "        else:\n",
        "            self.vectorizer = vectorizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = str(self.data.iloc[idx, 0])\n",
        "        x = \" \".join(x.split())\n",
        "        inputs = self.vectorizer.encode_plus(\n",
        "            x,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_seq_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        output = {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'id': idx\n",
        "        }\n",
        "        if self.mode != 'test':\n",
        "          output['targets'] = torch.tensor(self.label.iloc[idx]['y'], dtype=torch.float)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = BertDataset(data_dir, 'train', tokenizer_bert, MAX_SEQ_LENGTH)\n",
        "# val_ds = BertDataset(data_dir, 'val', tokenizer_bert, MAX_SEQ_LENGTH)\n",
        "test_ds = BertDataset(data_dir, 'test', train_ds.vectorizer, MAX_SEQ_LENGTH)"
      ],
      "metadata": {
        "id": "HuUlMUueuj9C"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "YQ1B65I0uj9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming len(train_ds) is the total number of samples in your dataset\n",
        "total_samples = len(train_ds)\n",
        "subset_size = 10\n",
        "# Create a random subset sampler\n",
        "subset_sampler = SubsetRandomSampler(range(subset_size))"
      ],
      "metadata": {
        "id": "7ntLYEDQuj9C"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader(train_dataset2, batch_size=256, sampler=train_sampler,collate_fn=collator.collate)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,num_workers=4)\n",
        "train_dl_short = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=subset_sampler,num_workers=4)\n",
        "# dev_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "# dev_dl_short = DataLoader(val_ds, batch_size=BATCH_SIZE, sampler=subset_sampler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b7488c-e637-48a2-ec88-e933204b229a",
        "id": "MKk-vZV8uj9D"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwwRJlKuj9D"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EepVflnIuj9E"
      },
      "source": [
        "### Functions Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "DBakEJ-Suj9E"
      },
      "outputs": [],
      "source": [
        "def evaluateBert(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    predicted_labels, correct_labels = [], []\n",
        "    with torch.inference_mode():\n",
        "      for step, data in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
        "          ids = data['ids'].to(device, dtype=torch.long)\n",
        "          mask = data['mask'].to(device, dtype=torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "          label_ids = data['id'].to(device, dtype=torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "          outputs = model(ids, mask, token_type_ids)\n",
        "          probas = torch.sigmoid(outputs)\n",
        "          loss = nn.BCEWithLogitsLoss()(outputs.squeeze(1), targets)\n",
        "          label_ids = label_ids.to('cpu').numpy()\n",
        "          predicted_labels += list(probas)\n",
        "          correct_labels += list(targets)\n",
        "\n",
        "          eval_loss += loss.item()\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "    correct_labels = np.array(correct_labels.to('cpu'))\n",
        "    predicted_labels = np.array(predicted_labels.to('cpu'))\n",
        "\n",
        "    return eval_loss, correct_labels, predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKj9SIzluj9F"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "IS58a7dNuj9F"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Umnqdo80uj9F"
      },
      "outputs": [],
      "source": [
        "num_train_steps = int(len(train_dl.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the data loader"
      ],
      "metadata": {
        "id": "HPOYEagcuj9G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "7b6c14b8b2de40e496abbb643e45620c",
            "1c5337038cfc42c0844507bdefff486e",
            "42d93c5291c845c38128d84436c3e399",
            "42c42eaf3e254e6bb706a7e750ed6e31",
            "fd4947389b344467a7b3a2937bdc31c0",
            "5040d9a1c86d4002b0df906e340e1be8",
            "831c3b53e8974c9b88a4d576fe8a4bd2",
            "7236f569f39e42e2ad52f8f3aa8751ec",
            "f886061e15d64595b82729c7c7c11f46",
            "53726b06094241d393220ef92da9209a",
            "74bc96e83dc04a719a9b10be75b7a4ba"
          ]
        },
        "outputId": "ce91c1cc-6f79-4e6a-9f2b-d48cfe936336",
        "id": "wrdyQt2Uuj9G"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b6c14b8b2de40e496abbb643e45620c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import trange\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "OUTPUT_DIR = \"/tmp/\"\n",
        "MODEL_FILE_NAME = \"bert_model_V0.bin\"\n",
        "PATIENCE = 3\n",
        "DL = train_dl_short\n",
        "# DL = train_dl\n",
        "# DL = train_dl\n",
        "loss_history = []\n",
        "no_improvement = 0\n",
        "i=0\n",
        "for epoch in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(tqdm(DL, desc=\"Training iteration\")):\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        optimizer.zero_grad()\n",
        "        print(loss)\n",
        "        loss = outputs[0]\n",
        "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "    # dev_loss, _, _ = evaluateBert(model, dev_dl)\n",
        "    # dev_loss, _, _ = evaluateBert(model, dev_dl_short)\n",
        "\n",
        "    print(\"Loss history:\", loss_history)\n",
        "    # print(\"Dev loss:\", dev_loss)\n",
        "    # print(\"Train loss:\", tr_loss.to('cpu')/int(NUM_TRAIN_EPOCHS))\n",
        "\n",
        "    if len(loss_history) == 0 or loss.item() < min(loss_history):\n",
        "        no_improvement = 0\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model\n",
        "        output_model_file = os.path.join(drive_dir, MODEL_FILE_NAME)\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "    else:\n",
        "        no_improvement += 1\n",
        "\n",
        "    if no_improvement >= PATIENCE:\n",
        "        print(\"No improvement on development set. Finish training.\")\n",
        "        break\n",
        "    i +=1\n",
        "    loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "MODEL_FILE_NAME =\"bert_model_V0.bin\"\n",
        "output_model_file = os.path.join(drive_dir + \"models/\", MODEL_FILE_NAME)\n",
        "torch.save(model_to_save.state_dict(), output_model_file)"
      ],
      "metadata": {
        "id": "On85j6lEuj9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzGpNnnkwt7j"
      },
      "source": [
        "## BERT DISTI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup parameters\n",
        "BATCH_SIZE = model_params['distilbert']['BATCH_SIZE']\n",
        "MAX_SEQ_LENGTH = model_params['distilbert']['MAX_SEQ_LENGTH']\n",
        "GRADIENT_ACCUMULATION_STEPS = model_params['distilbert']['GRADIENT_ACCUMULATION_STEPS']\n",
        "NUM_TRAIN_EPOCHS = model_params['distilbert']['NUM_TRAIN_EPOCHS']\n",
        "WARMUP_PROPORTION = model_params['distilbert']['WARMUP_PROPORTION']\n",
        "MAX_GRAD_NORM = model_params['distilbert']['MAX_GRAD_NORM']\n",
        "PATIENCE =model_params['distilbert']['PATIENCE']\n",
        "MAX_LEN = model_params['distilbert']['MAX_LEN']\n",
        "TRAIN_BATCH_SIZE = model_params['distilbert']['TRAIN_BATCH_SIZE']\n",
        "EPOCHS = model_params['distilbert']['EPOCHS']\n",
        "LEARNING_RATE = model_params['distilbert']['LEARNING_RATE']\n",
        "NUM_WORKERS = model_params['distilbert']['NUM_WORKERS']"
      ],
      "metadata": {
        "id": "Von_38mJWn0b"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some functions"
      ],
      "metadata": {
        "id": "zFFZ8hXxS9XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_BERT( model, optimizer, criterion, dataloader, scheduler,GRADIENT_ACCUMULATION_STEPS):\n",
        "  model.train()\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  losses, predictions, indices = [], [], []\n",
        "  for step, batch in enumerate(tqdm(dataloader, desc=\"Training iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids, idxs = batch\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids.float())\n",
        "        loss = outputs[0]\n",
        "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        pred = np.argmax(outputs['logits'].detach().to('cpu'), axis=1)\n",
        "        losses.extend([loss.item()] * len(label_ids))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idxs.tolist())\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "  pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "  dataset_loss = np.mean(losses)\n",
        "  dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy())\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  return dataset_loss, dataset_metric, losses"
      ],
      "metadata": {
        "id": "yal1CPwBbbWt"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_BERT_model(model, dataloader, criterion, cr =False):\n",
        "    \"\"\"\n",
        "        Evaluate the model on a given dataloader.\n",
        "        argument:\n",
        "            model [torch.nn.Module]: model to evaluate\n",
        "            dataloader [torch.utils.data.DataLoader]: dataloader on which to evaluate\n",
        "            criterion [torch.nn.modules.loss]: desired loss to compute\n",
        "        returns:\n",
        "            dataset_loss [float]: computed loss on the dataset\n",
        "            dataset_metric [float]: computed metric on the dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses, predictions, indices = [], [], []\n",
        "    eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    for batch in tqdm(dataloader, leave=False):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, y, idx = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "          res = model(input_ids, attention_mask=input_mask,\n",
        "                                          token_type_ids=segment_ids, labels=y)\n",
        "\n",
        "        pred = np.argmax(res['logits'].to('cpu'), axis=1)\n",
        "        # loss = criterion(pred.squeeze().to(device), y.squeeze())\n",
        "        eval_loss += res['loss'].mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        losses.extend([res['loss'].mean().item()] * len(y))\n",
        "        predictions.extend(pred.detach().squeeze().tolist())\n",
        "        indices.extend(idx.tolist())\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
        "    dataset_loss = np.mean(losses)\n",
        "    dataset_metric = worst_group_accuracy(pred_df, dataloader.dataset.label.copy(), cr)\n",
        "    return dataset_loss, dataset_metric, pred_df"
      ],
      "metadata": {
        "id": "WJ-jLVsGSrq5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "hMgIMnw-4Qrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Custom distilBERT\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "class DistilBERTClass(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.classifier = torch.nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "metadata": {
        "id": "e4YE5FMtzJ7S"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "CoYxrX56zztf"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhnUECEw3Mss"
      },
      "outputs": [],
      "source": [
        "BERT_MODEL ='distilbert-base-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274,
          "referenced_widgets": [
            "68a3b008d59a4833bffec55d7535fafc",
            "4bfa9de10bff40bda8df48aceb9d7ed5",
            "923fe15d837c44b18ab43dad1d61f0d5",
            "340f8b96e4314f4f8b453f0f7ef2f773",
            "b4d0413b2a9c4e888eb085c9189a5936",
            "30646e74411e40e9917fad157e1e8de7",
            "5ee7529c11a04f7580e14d40d4255ccd",
            "ab5d0f6476d54c8b800063a25f107f34",
            "973ed5e26327475aac1a9618bb142e96",
            "dc8dd9505da241d3ae7f76508161fd53",
            "3d85ab44044b44fa857be9a716e07334",
            "8ca3b1d15e404eda8afd9c6b0eb287ab",
            "145e17bb9a8b4c658d069a7fcb850aa0",
            "d54b1a1d2f61451c849a6181cfdb2f1b",
            "a37d1244698e40549cf2f36b88d83e42",
            "19c3ad2e621b498fb7aee3ef76f3825c",
            "652157e0dd3743f3880a4329e391152f",
            "2154222dca23469e83261c265b96ba5d",
            "65a170059df84988bd84a48aa1259194",
            "1f2da0e9877c4ffdb92007f3e6672f6a",
            "a66ff9966a2c4b278670914f33e9f69b",
            "5d2ddc5e4aab46419499eb06cba4acd0",
            "0a9d23d79a354bf5afe155c8071173eb",
            "22213a54984c4d3284f21d2428a9268c",
            "54b3774905984a53935afb59aaeabf65",
            "ef285bebf603453191eb205054856023",
            "ba83b1e341484054aefb008df2668fc4",
            "d9d122632d6c41b1b74719a621919e1a",
            "c49a44af47064543a31efdd07f51f64a",
            "fb8a171143774d1fb1dafad46ad32f9b",
            "d202a66d12dd488683d0e23bcbc101a6",
            "2aa160f148874a5a87cb0a8d767416a9",
            "dfa696befb174a3e8a3794b2df2df115",
            "9a341119f0c143beb88511410bf004be",
            "aa29e094e7a346cdb58aa3f6dca7f455",
            "8b36ceba59164c79b0cd6f19b72bfc3c",
            "acb6c726c8ec408380519406de939093",
            "ba02c229f799480a9ad0e6147303f46a",
            "d376ebb54f614a26a4d105c4e2a41c14",
            "07619291986d4ce581e01a5cf29a8300",
            "34efb72f4f0546e9a234488e00c9f060",
            "e19d65a6a0b542b2a6ccbc17b9ad4855",
            "09ba59a22d7c46d1878674516b136956",
            "b4210d1f3e1f4a3285ac1bf0d539cbd3"
          ]
        },
        "id": "EgogSD3fwsdz",
        "outputId": "70175e3b-c3b9-4735-f2f3-83644540426d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68a3b008d59a4833bffec55d7535fafc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ca3b1d15e404eda8afd9c6b0eb287ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a9d23d79a354bf5afe155c8071173eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a341119f0c143beb88511410bf004be"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "## BERT\n",
        "tokenizer_distil_bert =  DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "X1e42Lso4apK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtCnUOMr3GxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec45c7e1-5d58-4df1-f9c6-0d217c5ebded"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBERTClass(\n",
              "  (l1): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "model =  DistilBERTClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "cW7iLMQDLpox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "cwXxbICk4l4C"
      },
      "outputs": [],
      "source": [
        "#Working on it, still not rready\n",
        "class BertDistiDataset(Dataset):\n",
        "    def __init__(self, data_dir, mode, vectorizer=None, max_seq_length=100):\n",
        "        super(BertDistiDataset, self).__init__()\n",
        "        assert mode in ['train', 'val', 'test', 'augmented', 'augmented_clean','combined']\n",
        "        self.mode = mode\n",
        "        self.max_seq_length = max_seq_length\n",
        "        # load the data\n",
        "        self.data = pd.read_csv(os.path.join(data_dir, f'{mode}_x.csv'), index_col=0)\n",
        "\n",
        "        #clean data\n",
        "        if(mode != 'augmented_clean'):\n",
        "            #clean data\n",
        "            self.data.iloc[:, 0] = self.data.iloc[:, 0].apply(denoise_text,args_denoise_custom)\n",
        "            #remove stop words\n",
        "            # self.data.iloc[:, 0] = self.data.iloc[:, 0].apply(remove_stop_faster)\n",
        "\n",
        "        # load the labels if not the test set\n",
        "        if self.mode != 'test':\n",
        "            self.label = pd.read_csv(os.path.join(data_dir, f'{mode}_y.csv'))\n",
        "\n",
        "        # train the vectorizer if train set\n",
        "        if self.mode in ['train','augmented', 'augmented_clean','combined']:\n",
        "            self.vectorizer = vectorizer\n",
        "            # self.vectorizer.fit(self.data.values.flatten().tolist())\n",
        "        # otherwise use the vectorizer given as arguments (which was trained on the train set)\n",
        "        else:\n",
        "            self.vectorizer = vectorizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = str(self.data.iloc[idx, 0])\n",
        "        x = \" \".join(x.split())\n",
        "        inputs = self.vectorizer.encode_plus(\n",
        "            x,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_seq_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        output = {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'id': idx\n",
        "        }\n",
        "        if self.mode != 'test':\n",
        "          output['targets'] = torch.tensor(self.label.iloc[idx]['y'], dtype=torch.float)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = BertDistiDataset(data_dir, 'combined', tokenizer_distil_bert, MAX_SEQ_LENGTH)\n",
        "# val_ds = BertDataset(data_dir, 'val', train_ds.vectorizer, MAX_SEQ_LENGTH)\n",
        "test_ds = BertDistiDataset(data_dir, 'test', tokenizer_distil_bert, MAX_SEQ_LENGTH)"
      ],
      "metadata": {
        "id": "5WiVSNbRUqvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "FBTw9MOiV6R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming len(train_ds) is the total number of samples in your dataset\n",
        "total_samples = len(train_ds)\n",
        "subset_size = 10\n",
        "# Create a random subset sampler\n",
        "subset_sampler = SubsetRandomSampler(range(subset_size))"
      ],
      "metadata": {
        "id": "U8Cjzz7Varru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader(train_dataset2, batch_size=256, sampler=train_sampler,collate_fn=collator.collate)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,num_workers=4)\n",
        "train_dl_short = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=subset_sampler,num_workers=4)\n",
        "# dev_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "# dev_dl_short = DataLoader(val_ds, batch_size=BATCH_SIZE, sampler=subset_sampler)"
      ],
      "metadata": {
        "id": "ylCibVgyWVtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4427ec03-3608-40bf-d50e-0fd6d2a7ab81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = len(train_ds)\n",
        "to_boost = ['black','white','LGBTQ']\n",
        "val_samples_weight = get_weights_boosted(train_ds, categories, 1000,to_boost, 25)\n",
        "# val_samples_weight = get_weights_boosted(train_dataset2, categories, 1000,to_boost, 15)\n",
        "sampler_balanced = WeightedRandomSampler(val_samples_weight, n_samples, replacement=True )\n",
        "train_dataloader_bal_sampled = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler_balanced,num_workers=4, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8o2NNIRa2rr",
        "outputId": "e5eb9c5d-fafc-4ed0-f59a-bece04489297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKzUS6cLAH8P"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_weights = get_weights_balanced(train_ds, categories, 100000,[], 1)"
      ],
      "metadata": {
        "id": "gKgnW0c_0n0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_samples = 100000\n",
        "# sampler_balanced = WeightedRandomSampler(sample_weights, n_samples, replacement=False)\n",
        "# train_dl_bal_sampled = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler_balanced,num_workers=2, drop_last=True)"
      ],
      "metadata": {
        "id": "Dq1DILGM0tMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMrZVFQxADoV"
      },
      "source": [
        "### Functions Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFh-9nou99N_"
      },
      "outputs": [],
      "source": [
        "def evaluateBert(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    predicted_labels, correct_labels = [], []\n",
        "    with torch.inference_mode():\n",
        "      for step, data in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
        "          ids = data['ids'].to(device, dtype=torch.long)\n",
        "          mask = data['mask'].to(device, dtype=torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "          label_ids = data['id'].to(device, dtype=torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "          outputs = model(ids, mask, token_type_ids)\n",
        "          probas = torch.sigmoid(outputs)\n",
        "          loss = nn.BCEWithLogitsLoss()(outputs.squeeze(1), targets)\n",
        "          label_ids = label_ids.to('cpu').numpy()\n",
        "          predicted_labels += list(probas)\n",
        "          correct_labels += list(targets)\n",
        "\n",
        "          eval_loss += loss.item()\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "    correct_labels = np.array(correct_labels.to('cpu'))\n",
        "    predicted_labels = np.array(predicted_labels.to('cpu'))\n",
        "\n",
        "    return eval_loss, correct_labels, predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZW3iPFP-Ao-"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "1HEsmpJKq5Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRTN6giQ9-nP"
      },
      "outputs": [],
      "source": [
        "num_train_steps = int(len(train_dl.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the data loader"
      ],
      "metadata": {
        "id": "IwWfiA_eZ47m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dc2a1482d5324b4c96bdbefd6a41aced",
            "73a4af75b41a4fdb8c65f88916764d7f",
            "af1d6127c37e4fc1afb8413ffd8d8926",
            "d2f51a5606d442fca17860a6eda8fc1d",
            "d248e1f9d4b740d3a75bcdd7bc1f07ec",
            "55a39440312d4374afb6af8ffed6845c",
            "7056ba30095b4a3fb3bfdfc763c39eb5",
            "ad343f4d2f274ee9ae5fdd3425ad77f4",
            "9a495696a0c24be99bacfaec01536f13",
            "a685caf30fe5477f98ee2ff3b226feb0",
            "a15bdda05d5c49edad8f5256c99314dc",
            "d497edef737b40a4b2cd2af2bf4c18f5",
            "cb52be9d973b48a0b9ef0d9388d73167",
            "1e3327fe60fd41cf9888c33b7f9a3e1f",
            "46a025c330074e3c8e71858bdb3e5c69",
            "98cce7dfaccc436fa73eef93fcd00c5a",
            "4c69b8c0909f4a208bb4760cf6fa00ed",
            "92040912fc0b40e8aa805f7abbd683da",
            "c75686e054784781b2171f373004f49f",
            "dc3e1763eba049ee88a1d97dd7bf0d6b",
            "5f52bc58f57a4d76b5ce55c0700b28ad",
            "6831bbef64f941139db1468702ca28ec",
            "f3b67503a53d4884bc2dedcae5003bf0",
            "274f2912b57f40259f0851a09b76c9c7",
            "039f1ff25c764d63b304f32bdaeccf6f",
            "5087d1e6a3614ac990702eb7ae7e6447",
            "d7eac50d280248d580465240c242c568",
            "42f3ef69aede498f81729912ee1a4fab",
            "bd757a28cb5f4af1bacbbc66f5b67daa",
            "1c5e2545b9da45e3b4a336c0be7a3883",
            "e8f2202bcecf4972a28515e3dab659de",
            "59505855fd434979947c37d7e14a7a26",
            "ad38f76e8323402da67bdccbb5836493"
          ]
        },
        "id": "9VBPmS8_-NpH",
        "outputId": "37ab4ce0-3440-4804-dc16-07d124060918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training iteration:   0%|          | 0/19638 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc2a1482d5324b4c96bdbefd6a41aced"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss:  0.7008402943611145\n",
            "Epoch: 0, Loss:  0.21047565340995789\n",
            "Epoch: 0, Loss:  0.34670156240463257\n",
            "Epoch: 0, Loss:  0.15225005149841309\n",
            "Loss history: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [3:52:02<7:44:05, 13922.80s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training iteration:   0%|          | 0/19638 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d497edef737b40a4b2cd2af2bf4c18f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss:  0.45175155997276306\n",
            "Epoch: 1, Loss:  0.11689236015081406\n",
            "Epoch: 1, Loss:  0.025588421151041985\n",
            "Epoch: 1, Loss:  0.0047864485532045364\n",
            "Loss history: [0.053575675934553146]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [7:44:11<3:52:06, 13926.31s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training iteration:   0%|          | 0/19638 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3b67503a53d4884bc2dedcae5003bf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Loss:  0.026632683351635933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [8:33:20<4:16:40, 15400.07s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-eed6eec81f99>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnb_tr_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_tr_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import trange\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "OUTPUT_DIR = \"/tmp/\"\n",
        "MODEL_FILE_NAME = \"BERT_model_distil_.bin\"\n",
        "PATIENCE = 3\n",
        "# DL = train_dl_short\n",
        "DL = train_dataloader_bal_sampled\n",
        "# DL = train_dl\n",
        "loss_history = []\n",
        "no_improvement = 0\n",
        "i=0\n",
        "for epoch in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(tqdm(DL, desc=\"Training iteration\")):\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs.squeeze(1), targets)\n",
        "        if step%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # dev_loss, _, _ = evaluateBert(model, dev_dl)\n",
        "    # dev_loss, _, _ = evaluateBert(model, dev_dl_short)\n",
        "\n",
        "    print(\"Loss history:\", loss_history)\n",
        "    # print(\"Dev loss:\", dev_loss)\n",
        "    # print(\"Train loss:\", tr_loss.to('cpu')/int(NUM_TRAIN_EPOCHS))\n",
        "\n",
        "\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    output_model_file = os.path.join(drive_dir, f\"{MODEL_FILE_NAME}_epoch_{i}\")\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "    i +=1\n",
        "    loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "MODEL_FILE_NAME =\"BERT_model_distil_.bin\"\n",
        "output_model_file = os.path.join(drive_dir + \"models/\", MODEL_FILE_NAME)\n",
        "torch.save(model_to_save.state_dict(), output_model_file)"
      ],
      "metadata": {
        "id": "-skI-9VKNksy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models"
      ],
      "metadata": {
        "id": "Dd5e3BxDEeuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we load the already trained models, make predictions in the test data and combine their predictions."
      ],
      "metadata": {
        "id": "tgW9-K6vRWsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeq1etabzYEM",
        "outputId": "b3a5fffd-10bf-4214-cc7d-30a3920f8832"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel, BertTokenizer, BertModel, BertPreTrainedModel, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "GpAl1DkPrygs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert"
      ],
      "metadata": {
        "id": "wpKpTCkfpZU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_MODEL ='bert-base-uncased'\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(BERT_MODEL)\n",
        "model_state_dict = torch.load(drive_dir + saved_models['bert0'], map_location=device)\n",
        "models['bert0'] = BertForSequenceClassification.from_pretrained(BERT_MODEL, state_dict=model_state_dict, num_labels = 2)\n",
        "models['bert0'] .to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QqbD5r6EeVL",
        "outputId": "871fae47-b630-4619-826e-029269e66e45"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DISTIL_BERT_MODEL ='distilbert-base-uncased'\n",
        "tokenizer_distil_bert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
        "model_state_dict = torch.load(drive_dir + saved_models['distilbert'], map_location=device)\n",
        "models['distilbert'] = DistilBERTClass()\n",
        "models['distilbert'] .to(device)\n",
        "models['distilbert'].load_state_dict(model_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpYYkZ5Wq_HX",
        "outputId": "a1deab5c-c69e-4d14-b55a-42cad0a9c5ca"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "AvxuPzELpcPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_state_dict = torch.load(drive_dir+saved_models['lstm_yj'], map_location=device)\n",
        "models['lstm_yj'] = LSTM(model_params['lstm_yj']['INPUT_DIM'],model_params['lstm_yj']['EMBEDDING_DIM'], model_params['lstm_yj']['HIDDEN_DIM'],\n",
        "                         model_params['lstm_yj']['N_LAYERS'], model_params['lstm_yj']['OUTPUT_DIM'],model_params['lstm_yj']['DROPOUT']\n",
        "                         )\n",
        "models['lstm_yj'].load_state_dict(model_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Lkzq3lpQSc",
        "outputId": "3c4e7397-1d66-4685-b574-9412728b2f6d"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data to predict"
      ],
      "metadata": {
        "id": "YQejvSvBrxY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds_BERT = BertDataset(data_dir, 'test', tokenizer_bert, model_params['bert0']['MAX_SEQ_LENGTH'])\n",
        "val_ds_DISTIBERT = BertDistiDataset(data_dir, 'test', tokenizer_distil_bert, model_params['distilbert']['MAX_SEQ_LENGTH'])\n",
        "val_ds_LSTM = LSTMDataset(data_dir, 'test', text_pipeline_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR9SuQsdr265",
        "outputId": "34274ee8-de0a-4d01-a31a-9720604b4830"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-10-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-10-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-10-6fc32031401b>:3: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrXAv7hbVyZD"
      },
      "source": [
        "# Making predictions on Test by voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSiizvRpr_5J"
      },
      "source": [
        "In this approach we will predict the label from each of the models and generate a majority voting to get the last prediction.\n",
        "- ID: with the id of each prediction (do not shuffle to not mix things up)\n",
        "- pred: the prediction of the model (thresholded or not)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collator = Collator(3)\n",
        "test_dl_LSTM = DataLoader(val_ds_LSTM, batch_size=64, shuffle=False, collate_fn=collator.collate, num_workers=2)\n",
        "test_dl_distilbert = DataLoader(val_ds_DISTIBERT, batch_size=64, shuffle=False, num_workers=2)\n",
        "test_dl_bert = DataLoader(val_ds_BERT, batch_size=64, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "CFD0_WF7wMkS"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXoPeUTjsxNu"
      },
      "outputs": [],
      "source": [
        "#LSTM\n",
        "models['lstm_yj'].eval()\n",
        "test_predictions, indices, probs = [], [], []\n",
        "for x,y, idx in tqdm(test_dl_LSTM, leave=False):\n",
        "    with torch.no_grad():\n",
        "        output = models['lstm_yj'](x.to(device))\n",
        "        pred = (output.squeeze() > 0.5).int()\n",
        "    test_predictions.extend(pred.tolist())\n",
        "    indices.extend(idx.tolist())\n",
        "    probs.extend(output.squeeze().tolist())\n",
        "pred_LSTM_df = pd.DataFrame({'ID': indices, 'pred': test_predictions, 'probs':probs})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT\n",
        "models['bert0'].eval()\n",
        "test_predictions, indices,probs = [], [], []\n",
        "for batch in tqdm(test_dl_bert, leave=False):\n",
        "    ids = batch['ids'].to(device, dtype = torch.long)\n",
        "    mask = batch['mask'].to(device, dtype = torch.long)\n",
        "    token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
        "    idx = batch['id']\n",
        "    with torch.no_grad():\n",
        "        outputs = models['bert0'](ids, attention_mask=mask,\n",
        "                                          token_type_ids=token_type_ids)\n",
        "    sigmoids = F.sigmoid(outputs['logits']).squeeze(1).to('cpu')\n",
        "    pred = np.argmax(sigmoids, axis=1)\n",
        "    test_predictions.extend(pred.tolist())\n",
        "    indices.extend(idx.tolist())\n",
        "    probs.extend(sigmoids[:,1].tolist()) # The output is 2 dimensional, we need probability of class 1\n",
        "pred_BERT_df = pd.DataFrame({'ID': indices, 'pred': test_predictions, 'probs':probs})"
      ],
      "metadata": {
        "id": "DfqI1E7uxTDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models['distilbert'].eval()\n",
        "test_predictions, indices, probs = [], [], []\n",
        "for batch in tqdm(test_dl_bert, leave=False):\n",
        "    ids = batch['ids'].to(device, dtype = torch.long)\n",
        "    mask = batch['mask'].to(device, dtype = torch.long)\n",
        "    token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
        "    idx = batch['id']\n",
        "    with torch.no_grad():\n",
        "        outputs = models['distilbert'](ids, attention_mask=mask,\n",
        "                                          token_type_ids=token_type_ids)\n",
        "    sigmoids = F.sigmoid(outputs).squeeze(1).to('cpu')\n",
        "    pred = np.where(sigmoids> 0.5,1,0)\n",
        "    test_predictions.extend(pred.tolist())\n",
        "    indices.extend(idx.tolist())\n",
        "    probs.extend(sigmoids.tolist())\n",
        "pred_DISTILBERT_df = pd.DataFrame({'ID': indices, 'pred': test_predictions, 'probs':probs})"
      ],
      "metadata": {
        "id": "UCSCQg_kyY_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Voting"
      ],
      "metadata": {
        "id": "HfZYKZ5-4c3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_final = np.where((pred_DISTILBERT_df['pred']+pred_BERT_df['pred']+pred_LSTM_df['pred'])>1,1,0)"
      ],
      "metadata": {
        "id": "7aUEBXNN4R2Z"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzlemJhdFwNo"
      },
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame({'ID': indices, 'pred': pred_final})\n",
        "pred_df.to_csv(data_dir+'/prediction_final_bert0-distilbert-lstm.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01db701dc76340d9b69948b350bc8776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3442a2c32a14eefa1080d265053b0f5",
              "IPY_MODEL_5f911f2d04ea4640a57be12511a75eeb",
              "IPY_MODEL_34cacc52efb2418e88ada1cb064ffe48"
            ],
            "layout": "IPY_MODEL_85d7e461f17e45f5a16d4ee29af5d5f7"
          }
        },
        "20bc98e5655d44359eaf33c94484481b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "283470ab74b34fe99d7527ce5d3d062b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d6f42abe847480ca96e40a4ce34a724": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34cacc52efb2418e88ada1cb064ffe48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d6f42abe847480ca96e40a4ce34a724",
            "placeholder": "​",
            "style": "IPY_MODEL_f640c1717cd54ce88cc25dd5783de6bf",
            "value": " 1410/1412 [00:25&lt;00:00, 53.02it/s]"
          }
        },
        "3fa5cbc37857433c8191468d19768894": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51af2460702e44b4a67393159d9cf75f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f911f2d04ea4640a57be12511a75eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa5cbc37857433c8191468d19768894",
            "max": 1412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_283470ab74b34fe99d7527ce5d3d062b",
            "value": 1412
          }
        },
        "85d7e461f17e45f5a16d4ee29af5d5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "b3442a2c32a14eefa1080d265053b0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51af2460702e44b4a67393159d9cf75f",
            "placeholder": "​",
            "style": "IPY_MODEL_20bc98e5655d44359eaf33c94484481b",
            "value": "100%"
          }
        },
        "f640c1717cd54ce88cc25dd5783de6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b6c14b8b2de40e496abbb643e45620c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c5337038cfc42c0844507bdefff486e",
              "IPY_MODEL_42d93c5291c845c38128d84436c3e399",
              "IPY_MODEL_42c42eaf3e254e6bb706a7e750ed6e31"
            ],
            "layout": "IPY_MODEL_fd4947389b344467a7b3a2937bdc31c0"
          }
        },
        "1c5337038cfc42c0844507bdefff486e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5040d9a1c86d4002b0df906e340e1be8",
            "placeholder": "​",
            "style": "IPY_MODEL_831c3b53e8974c9b88a4d576fe8a4bd2",
            "value": "Training iteration:   0%"
          }
        },
        "42d93c5291c845c38128d84436c3e399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7236f569f39e42e2ad52f8f3aa8751ec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f886061e15d64595b82729c7c7c11f46",
            "value": 0
          }
        },
        "42c42eaf3e254e6bb706a7e750ed6e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53726b06094241d393220ef92da9209a",
            "placeholder": "​",
            "style": "IPY_MODEL_74bc96e83dc04a719a9b10be75b7a4ba",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "fd4947389b344467a7b3a2937bdc31c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5040d9a1c86d4002b0df906e340e1be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "831c3b53e8974c9b88a4d576fe8a4bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7236f569f39e42e2ad52f8f3aa8751ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f886061e15d64595b82729c7c7c11f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53726b06094241d393220ef92da9209a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74bc96e83dc04a719a9b10be75b7a4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}